{"cells":[{"cell_type":"markdown","metadata":{"id":"3ezJ3Y7XRUnj"},"source":["# Stable Baselines3 Tutorial - Gym wrappers, saving and loading models\n","\n","Github repo: https://github.com/araffin/rl-tutorial-jnrr19/tree/sb3/\n","\n","Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n","\n","Documentation: https://stable-baselines3.readthedocs.io/en/master/\n","\n","SB3-Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n","\n","RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n","\n","\n","## Introduction\n","\n","In this notebook, you will learn how to use *Gym Wrappers* which allow to do monitoring, normalization, limit the number of steps, feature augmentation, ...\n","\n","\n","You will also see the *loading* and *saving* functions, and how to read the outputted files for possible exporting.\n","\n","## Install Dependencies and Stable Baselines3 Using Pip"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"aq2YutzjbyOQ","executionInfo":{"status":"ok","timestamp":1688973083196,"user_tz":-480,"elapsed":2,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}}},"outputs":[],"source":["# for autoformatting\n","# %load_ext jupyter_black"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"YFdlFByORUnl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688973097241,"user_tz":-480,"elapsed":13692,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}},"outputId":"ea24370f-3144-4f74-c602-4e2dda06ce3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.1.1)\n","Requirement already satisfied: stable-baselines3[extra]>=2.0.0a4 in /usr/local/lib/python3.10/dist-packages (2.0.0)\n","Requirement already satisfied: gymnasium==0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (0.28.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (1.22.4)\n","Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (2.0.1+cu118)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (2.2.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (1.5.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (3.7.1)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (4.7.0.72)\n","Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (2.12.3)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (5.9.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (4.65.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (13.4.2)\n","Requirement already satisfied: shimmy[atari]~=0.2.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (0.2.1)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (8.4.0)\n","Requirement already satisfied: autorom[accept-rom-license]~=0.6.0 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (0.6.1)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (2.4.0)\n","Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->stable-baselines3[extra]>=2.0.0a4) (1.0.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->stable-baselines3[extra]>=2.0.0a4) (4.6.3)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->stable-baselines3[extra]>=2.0.0a4) (0.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]>=2.0.0a4) (8.1.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]>=2.0.0a4) (2.27.1)\n","Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]>=2.0.0a4) (0.6.1)\n","Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]~=0.2.1->stable-baselines3[extra]>=2.0.0a4) (0.8.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.56.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.4.3)\n","Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.20.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (67.7.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (2.3.6)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (0.40.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3[extra]>=2.0.0a4) (3.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3[extra]>=2.0.0a4) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3[extra]>=2.0.0a4) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3[extra]>=2.0.0a4) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3[extra]>=2.0.0a4) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11->stable-baselines3[extra]>=2.0.0a4) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11->stable-baselines3[extra]>=2.0.0a4) (16.0.6)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (4.40.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (23.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (3.1.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]>=2.0.0a4) (2022.7.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]>=2.0.0a4) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]>=2.0.0a4) (2.14.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=0.2.1->stable-baselines3[extra]>=2.0.0a4) (5.12.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (0.3.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.16.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.3.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a4) (0.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]>=2.0.0a4) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]>=2.0.0a4) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]>=2.0.0a4) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]>=2.0.0a4) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11->stable-baselines3[extra]>=2.0.0a4) (1.3.0)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.2.2)\n"]}],"source":["!pip install swig\n","!pip install \"stable-baselines3[extra]>=2.0.0a4\""]},{"cell_type":"code","execution_count":9,"metadata":{"id":"grXe85G9RUnp","executionInfo":{"status":"ok","timestamp":1688973101302,"user_tz":-480,"elapsed":2,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}}},"outputs":[],"source":["import gymnasium as gym\n","from stable_baselines3 import A2C, SAC, PPO, TD3"]},{"cell_type":"markdown","metadata":{"id":"hMPAn1SRd32f"},"source":["# Saving and loading\n","\n","Saving and loading stable-baselines models is straightforward: you can directly call `.save()` and `.load()` on the models."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"vBNFnN4Gd32g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688973122356,"user_tz":-480,"elapsed":19308,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}},"outputId":"4bf92482-2766-4b2d-c66d-a93902c8d25c"},"outputs":[{"output_type":"stream","name":"stdout","text":["pre saved (array([0.14078338], dtype=float32), None)\n","loaded (array([0.14078338], dtype=float32), None)\n"]}],"source":["import os\n","\n","# Create save dir\n","save_dir = \"/tmp/gym/\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","model = PPO(\"MlpPolicy\", \"Pendulum-v1\", verbose=0).learn(8_000)\n","# The model will be saved under PPO_tutorial.zip\n","model.save(f\"{save_dir}/PPO_tutorial\")\n","\n","# sample an observation from the environment\n","obs = model.env.observation_space.sample()\n","\n","# Check prediction before saving\n","print(\"pre saved\", model.predict(obs, deterministic=True))\n","\n","del model  # delete trained model to demonstrate loading\n","\n","loaded_model = PPO.load(f\"{save_dir}/PPO_tutorial\")\n","# Check that the prediction is the same after loading (for the same observation)\n","print(\"loaded\", loaded_model.predict(obs, deterministic=True))"]},{"cell_type":"code","source":["# !pwd\n","!ls -l /tmp/gym"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LWsFCqOQfFvs","executionInfo":{"status":"ok","timestamp":1688973126454,"user_tz":-480,"elapsed":554,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}},"outputId":"1254ecb3-b320-4d57-b179-833102f8ad05"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["total 224\n","-rw-r--r-- 1 root root  91227 Jul 10 07:07 A2C_tutorial.zip\n","-rw-r--r-- 1 root root 134298 Jul 10 07:12 PPO_tutorial.zip\n"]}]},{"cell_type":"markdown","metadata":{"id":"gXWPrVqId32o"},"source":["Saving in stable-baselines is quite powerful, as you save the training hyperparameters, with the current weights. This means in practice, you can simply load a custom model, without redefining the parameters, and continue learning.\n","\n","The loading function can also update the model's class variables when loading."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"LCtxrAbXd32q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688973155854,"user_tz":-480,"elapsed":26108,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}},"outputId":"b1e323c7-cc68-438e-ba46-64c0a5cf1df5"},"outputs":[{"output_type":"stream","name":"stdout","text":["loaded: gamma=0.9, n_steps=20\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 740      |\n","|    iterations         | 100      |\n","|    time_elapsed       | 2        |\n","|    total_timesteps    | 2000     |\n","| train/                |          |\n","|    entropy_loss       | -1.42    |\n","|    explained_variance | 0.0133   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 499      |\n","|    policy_loss        | -22.8    |\n","|    std                | 1        |\n","|    value_loss         | 954      |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 737      |\n","|    iterations         | 200      |\n","|    time_elapsed       | 5        |\n","|    total_timesteps    | 4000     |\n","| train/                |          |\n","|    entropy_loss       | -1.43    |\n","|    explained_variance | 0.00339  |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 599      |\n","|    policy_loss        | -17.5    |\n","|    std                | 1.01     |\n","|    value_loss         | 367      |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 684      |\n","|    iterations         | 300      |\n","|    time_elapsed       | 8        |\n","|    total_timesteps    | 6000     |\n","| train/                |          |\n","|    entropy_loss       | -1.44    |\n","|    explained_variance | 0.000972 |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 699      |\n","|    policy_loss        | -35.2    |\n","|    std                | 1.02     |\n","|    value_loss         | 858      |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 695      |\n","|    iterations         | 400      |\n","|    time_elapsed       | 11       |\n","|    total_timesteps    | 8000     |\n","| train/                |          |\n","|    entropy_loss       | -1.45    |\n","|    explained_variance | 0.111    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 799      |\n","|    policy_loss        | -27.4    |\n","|    std                | 1.04     |\n","|    value_loss         | 700      |\n","------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["<stable_baselines3.a2c.a2c.A2C at 0x7fd7c9c713c0>"]},"metadata":{},"execution_count":12}],"source":["import os\n","from stable_baselines3.common.vec_env import DummyVecEnv\n","\n","# Create save dir\n","save_dir = \"/tmp/gym/\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","model = A2C(\"MlpPolicy\", \"Pendulum-v1\", verbose=0, gamma=0.9, n_steps=20).learn(8000)\n","# The model will be saved under A2C_tutorial.zip\n","model.save(f\"{save_dir}/A2C_tutorial\")\n","\n","del model  # delete trained model to demonstrate loading\n","\n","# load the model, and when loading set verbose to 1\n","loaded_model = A2C.load(f\"{save_dir}/A2C_tutorial\", verbose=1)\n","\n","# show the save hyperparameters\n","print(f\"loaded: gamma={loaded_model.gamma}, n_steps={loaded_model.n_steps}\")\n","\n","# as the environment is not serializable, we need to set a new instance of the environment\n","loaded_model.set_env(DummyVecEnv([lambda: gym.make(\"Pendulum-v1\")]))\n","# and continue training\n","loaded_model.learn(8_000)"]},{"cell_type":"markdown","metadata":{"id":"hKwupU-Jgxjm"},"source":["# Gym and VecEnv wrappers"]},{"cell_type":"markdown","metadata":{"id":"ds4AAfmISQIA"},"source":["## Anatomy of a gym wrapper"]},{"cell_type":"markdown","metadata":{"id":"gnTS9e9hTzZZ"},"source":["A gym wrapper follows the [gym](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html) interface: it has a `reset()` and `step()` method.\n","\n","Because a wrapper is *around* an environment, we can access it with `self.env`, this allow to easily interact with it without modifying the original env.\n","There are many wrappers that have been predefined, for a complete list refer to [gym documentation](https://gymnasium.farama.org/api/wrappers/)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"hYo0C0TQSL3c","executionInfo":{"status":"ok","timestamp":1688973636471,"user_tz":-480,"elapsed":383,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}}},"outputs":[],"source":["class CustomWrapper(gym.Wrapper):\n","    \"\"\"\n","    :param env: (gym.Env) Gym environment that will be wrapped\n","    \"\"\"\n","\n","    def __init__(self, env):\n","        # Call the parent constructor, so we can access self.env later\n","        super().__init__(env)\n","\n","    def reset(self, **kwargs):\n","        \"\"\"\n","        Reset the environment\n","        \"\"\"\n","        obs, info = self.env.reset(**kwargs)\n","\n","        return obs, info\n","\n","    def step(self, action):\n","        \"\"\"\n","        :param action: ([float] or int) Action taken by the agent\n","        :return: (np.ndarray, float, bool, bool, dict) observation, reward, is this a final state (episode finished),\n","        is the max number of steps reached (episode finished artificially), additional informations\n","        \"\"\"\n","        obs, reward, terminated, truncated, info = self.env.step(action)\n","        return obs, reward, terminated, truncated, info"]},{"cell_type":"markdown","metadata":{"id":"4zeGuyICUN26"},"source":["## First example: limit the episode length\n","\n","One practical use case of a wrapper is when you want to limit the number of steps by episode, for that you will need to overwrite the `done` signal when the limit is reached. It is also a good practice to pass that information in the `info` dictionary."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"Eb2U4_K6SNUx","executionInfo":{"status":"ok","timestamp":1688973742552,"user_tz":-480,"elapsed":343,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}}},"outputs":[],"source":["class TimeLimitWrapper(gym.Wrapper):\n","    \"\"\"\n","    :param env: (gym.Env) Gym environment that will be wrapped\n","    :param max_steps: (int) Max number of steps per episode\n","    \"\"\"\n","\n","    def __init__(self, env, max_steps=100):\n","        # Call the parent constructor, so we can access self.env later\n","        super(TimeLimitWrapper, self).__init__(env)\n","        self.max_steps = max_steps\n","        # Counter of steps per episode\n","        self.current_step = 0\n","\n","    def reset(self, **kwargs):\n","        \"\"\"\n","        Reset the environment\n","        \"\"\"\n","        # Reset the counter\n","        self.current_step = 0\n","        return self.env.reset(**kwargs)\n","\n","    def step(self, action):\n","        \"\"\"\n","        :param action: ([float] or int) Action taken by the agent\n","        :return: (np.ndarray, float, bool, bool, dict) observation, reward, is the episode over?, additional informations\n","        \"\"\"\n","        self.current_step += 1\n","        obs, reward, terminated, truncated, info = self.env.step(action)\n","        # Overwrite the truncation signal when when the number of steps reaches the maximum\n","        if self.current_step >= self.max_steps:\n","            truncated = True\n","        return obs, reward, terminated, truncated, info"]},{"cell_type":"markdown","metadata":{"id":"oZufaUJwVM9w"},"source":["#### Test the wrapper"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"szZ43D5PVB07","executionInfo":{"status":"ok","timestamp":1688973754707,"user_tz":-480,"elapsed":324,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}}},"outputs":[],"source":["from gymnasium.envs.classic_control.pendulum import PendulumEnv\n","\n","# Here we create the environment directly because gym.make() already wrap the environment in a TimeLimit wrapper otherwise\n","env = PendulumEnv()\n","# Wrap the environment\n","env = TimeLimitWrapper(env, max_steps=100)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"cencka9iVg9V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688973801371,"user_tz":-480,"elapsed":395,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}},"outputId":"5b79d4a2-36bc-4eb2-c3b4-e97a251babe7"},"outputs":[{"output_type":"stream","name":"stdout","text":["obs: [-0.64142185 -0.7671884  -0.9077568 ]\n","random_action: [1.7621384]\n","random_action: [-0.5693424]\n","random_action: [-1.2979594]\n","random_action: [0.7534419]\n","random_action: [-0.943226]\n","random_action: [-1.8573998]\n","random_action: [-0.96601194]\n","random_action: [-0.5377286]\n","random_action: [1.7353821]\n","random_action: [1.0838821]\n","random_action: [0.64537007]\n","random_action: [-0.7318887]\n","random_action: [0.5567026]\n","random_action: [-0.5394251]\n","random_action: [0.8593188]\n","random_action: [-1.8811326]\n","random_action: [1.5269382]\n","random_action: [0.81244224]\n","random_action: [-0.09148755]\n","random_action: [0.77200884]\n","random_action: [-0.69063735]\n","random_action: [-1.538656]\n","random_action: [0.03880705]\n","random_action: [-0.92049295]\n","random_action: [1.4014941]\n","random_action: [-0.71012855]\n","random_action: [0.65147364]\n","random_action: [-0.3777858]\n","random_action: [-0.4083383]\n","random_action: [0.61120707]\n","random_action: [0.97940344]\n","random_action: [-0.17602655]\n","random_action: [1.5133942]\n","random_action: [0.00090826]\n","random_action: [-1.4107255]\n","random_action: [-0.24201803]\n","random_action: [-1.292436]\n","random_action: [1.593141]\n","random_action: [1.9449524]\n","random_action: [0.98526347]\n","random_action: [-0.54366857]\n","random_action: [0.9489005]\n","random_action: [-0.09817151]\n","random_action: [0.26170316]\n","random_action: [-1.3284812]\n","random_action: [-0.64252174]\n","random_action: [-0.04810119]\n","random_action: [-1.5704385]\n","random_action: [1.8992795]\n","random_action: [-1.3334477]\n","random_action: [-0.52881503]\n","random_action: [-0.9373346]\n","random_action: [-1.8902345]\n","random_action: [0.13174425]\n","random_action: [1.6040536]\n","random_action: [-1.1881704]\n","random_action: [-1.646669]\n","random_action: [-1.977873]\n","random_action: [-0.35879716]\n","random_action: [-0.51011026]\n","random_action: [1.3799878]\n","random_action: [-1.5577855]\n","random_action: [-0.47854412]\n","random_action: [0.27759564]\n","random_action: [-0.28827837]\n","random_action: [-0.8488791]\n","random_action: [-0.72526383]\n","random_action: [0.16306996]\n","random_action: [0.9536719]\n","random_action: [-1.8651937]\n","random_action: [1.0044042]\n","random_action: [-1.4268705]\n","random_action: [-1.3172125]\n","random_action: [0.20933366]\n","random_action: [0.5430582]\n","random_action: [-0.26115128]\n","random_action: [-0.85031116]\n","random_action: [-1.6221883]\n","random_action: [1.1545565]\n","random_action: [1.1872736]\n","random_action: [1.8563597]\n","random_action: [-0.94850624]\n","random_action: [0.36547095]\n","random_action: [0.43967053]\n","random_action: [1.1411929]\n","random_action: [-0.87492007]\n","random_action: [-0.0698243]\n","random_action: [1.5647554]\n","random_action: [1.8038282]\n","random_action: [1.3378419]\n","random_action: [-0.9990237]\n","random_action: [-0.794907]\n","random_action: [-0.47096902]\n","random_action: [-1.8942778]\n","random_action: [-1.8149562]\n","random_action: [1.5826962]\n","random_action: [-0.2417536]\n","random_action: [0.79650766]\n","random_action: [0.19533448]\n","random_action: [-0.82279444]\n","100 {}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["obs, _ = env.reset()\n","\n","print(f\"obs: {obs}\")\n","\n","done = False\n","n_steps = 0\n","while not done:\n","    # Take random actions\n","    random_action = env.action_space.sample()\n","\n","    print(f\"random_action: {random_action}\")\n","\n","    obs, reward, terminated, truncated, info = env.step(random_action)\n","    done = terminated or truncated\n","    n_steps += 1\n","\n","print(n_steps, info)"]},{"cell_type":"markdown","metadata":{"id":"jkMYA63sV9aA"},"source":["In practice, `gym` already have a wrapper for that named `TimeLimit` (`gym.wrappers.TimeLimit`) that is used by most environments."]},{"cell_type":"markdown","metadata":{"id":"VIIJbSyQW9R-"},"source":["## Second example: normalize actions\n","\n","It is usually a good idea to normalize observations and actions before giving it to the agent, this prevents this [hard to debug issue](https://github.com/hill-a/stable-baselines/issues/473).\n","\n","In this example, we are going to normalize the action space of *Pendulum-v1* so it lies in [-1, 1] instead of [-2, 2].\n","\n","Note: here we are dealing with continuous actions, hence the `gym.Box` space"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"F5E6kZfzW8vy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688974169848,"user_tz":-480,"elapsed":531,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}},"outputId":"da8fe93d-24a2-475d-bdb8-cc5e95bcc2c8"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["import numpy as np\n","\n","\n","class NormalizeActionWrapper(gym.Wrapper):\n","    \"\"\"\n","    :param env: (gym.Env) Gym environment that will be wrapped\n","    \"\"\"\n","\n","    def __init__(self, env):\n","        # Retrieve the action space\n","        action_space = env.action_space\n","        assert isinstance(\n","            action_space, gym.spaces.Box\n","        ), \"This wrapper only works with continuous action space (spaces.Box)\"\n","        # Retrieve the max/min values\n","        self.low, self.high = action_space.low, action_space.high\n","\n","        # We modify the action space, so all actions will lie in [-1, 1]\n","        env.action_space = gym.spaces.Box(\n","            low=-1, high=1, shape=action_space.shape, dtype=np.float32\n","        )\n","\n","        # Call the parent constructor, so we can access self.env later\n","        super(NormalizeActionWrapper, self).__init__(env)\n","\n","    def rescale_action(self, scaled_action):\n","        \"\"\"\n","        Rescale the action from [-1, 1] to [low, high]\n","        (no need for symmetric action space)\n","        :param scaled_action: (np.ndarray)\n","        :return: (np.ndarray)\n","        \"\"\"\n","\n","        print(f\"scaled_action: {scaled_action}\")\n","\n","        return self.low + (0.5 * (scaled_action + 1.0) * (self.high - self.low))\n","\n","    def reset(self, **kwargs):\n","        \"\"\"\n","        Reset the environment\n","        \"\"\"\n","        return self.env.reset(**kwargs)\n","\n","    def step(self, action):\n","        \"\"\"\n","        :param action: ([float] or int) Action taken by the agent\n","        :return: (np.ndarray, float,bool, bool, dict) observation, reward, final state? truncated?, additional informations\n","        \"\"\"\n","        print(f\"step\")\n","\n","        # Rescale action from [-1, 1] to original [low, high] interval\n","        rescaled_action = self.rescale_action(action)\n","        obs, reward, terminated, truncated, info = self.env.step(rescaled_action)\n","        return obs, reward, terminated, truncated, info"]},{"cell_type":"markdown","metadata":{"id":"TmJ0eahNaR6K"},"source":["#### Test before rescaling actions"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"UEnjBwisaQIx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688974169849,"user_tz":-480,"elapsed":4,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}},"outputId":"4e764f77-2438-4ad8-d5b0-bcc34d55c33a"},"outputs":[{"output_type":"stream","name":"stdout","text":["[-2.]\n","[1.7500577]\n","[-0.6480332]\n","[1.5230565]\n","[0.5828057]\n","[-0.9401164]\n","[0.38591135]\n","[-1.5917381]\n","[1.3897209]\n","[-1.7882981]\n","[-0.3596684]\n"]}],"source":["original_env = gym.make(\"Pendulum-v1\")\n","\n","print(original_env.action_space.low)\n","for _ in range(10):\n","    print(original_env.action_space.sample())"]},{"cell_type":"markdown","metadata":{"id":"jvcll2L3afVd"},"source":["#### Test the NormalizeAction wrapper"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"WsCM9AUGaeBN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688974171637,"user_tz":-480,"elapsed":2,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}},"outputId":"6e698515-2ae3-4d2b-ca5d-715e5a023330"},"outputs":[{"output_type":"stream","name":"stdout","text":["[-1.]\n","[-0.9170779]\n","[0.5489911]\n","[0.25602958]\n","[-0.03258502]\n","[-0.48956802]\n","[0.5583986]\n","[0.3103775]\n","[-0.27756217]\n","[0.91809154]\n","[0.7701595]\n"]}],"source":["env = NormalizeActionWrapper(gym.make(\"Pendulum-v1\"))\n","\n","print(env.action_space.low)\n","\n","for _ in range(10):\n","    print(env.action_space.sample())"]},{"cell_type":"markdown","metadata":{"id":"V5h5kk2mbGNs"},"source":["#### Test with a RL algorithm\n","\n","We are going to use the Monitor wrapper of stable baselines, which allow to monitor training stats (mean episode reward, mean episode length)"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"R9FNCN8ybOVU","executionInfo":{"status":"ok","timestamp":1688974200847,"user_tz":-480,"elapsed":388,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}}},"outputs":[],"source":["from stable_baselines3.common.monitor import Monitor\n","from stable_baselines3.common.vec_env import DummyVecEnv"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"wutM3c1GbfGP","executionInfo":{"status":"ok","timestamp":1688974205320,"user_tz":-480,"elapsed":412,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}}},"outputs":[],"source":["env = Monitor(gym.make(\"Pendulum-v1\"))\n","env = DummyVecEnv([lambda: env])"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"8cxnE5bdaQ_3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688974497869,"user_tz":-480,"elapsed":2447,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}},"outputId":"f70da2d3-fd25-48b1-dc12-4a1e051e4cf6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda device\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 200       |\n","|    ep_rew_mean        | -1.33e+03 |\n","| time/                 |           |\n","|    fps                | 501       |\n","|    iterations         | 100       |\n","|    time_elapsed       | 0         |\n","|    total_timesteps    | 500       |\n","| train/                |           |\n","|    entropy_loss       | -1.43     |\n","|    explained_variance | -0.325    |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 99        |\n","|    policy_loss        | -20.1     |\n","|    std                | 1.01      |\n","|    value_loss         | 220       |\n","-------------------------------------\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 200       |\n","|    ep_rew_mean        | -1.19e+03 |\n","| time/                 |           |\n","|    fps                | 504       |\n","|    iterations         | 200       |\n","|    time_elapsed       | 1         |\n","|    total_timesteps    | 1000      |\n","| train/                |           |\n","|    entropy_loss       | -1.44     |\n","|    explained_variance | -0.0139   |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 199       |\n","|    policy_loss        | -23.7     |\n","|    std                | 1.03      |\n","|    value_loss         | 396       |\n","-------------------------------------\n"]}],"source":["model = A2C(\"MlpPolicy\", env, verbose=1).learn(int(1000))"]},{"cell_type":"markdown","metadata":{"id":"EJFSM-Drb3Wc"},"source":["With the action wrapper"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"GszFZthob2wM","executionInfo":{"status":"ok","timestamp":1688974817200,"user_tz":-480,"elapsed":337,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}}},"outputs":[],"source":["normalized_env = Monitor(gym.make(\"Pendulum-v1\"))\n","# Note that we can use multiple wrappers\n","normalized_env = NormalizeActionWrapper(normalized_env)\n","normalized_env = DummyVecEnv([lambda: normalized_env])"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"wrKJEO4NcIMd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688974820701,"user_tz":-480,"elapsed":3096,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}},"outputId":"a5ec81fd-974f-404a-bbd4-cf35f4b1f316"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda device\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.12737146]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.23767176]\n","step\n","scaled_action: [-0.6407828]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.46790993]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.18184617]\n","step\n","scaled_action: [0.01184371]\n","step\n","scaled_action: [-0.01135069]\n","step\n","scaled_action: [0.406163]\n","step\n","scaled_action: [-0.91658497]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.6070815]\n","step\n","scaled_action: [-0.48924893]\n","step\n","scaled_action: [0.668292]\n","step\n","scaled_action: [0.06516597]\n","step\n","scaled_action: [0.14412935]\n","step\n","scaled_action: [-0.6238987]\n","step\n","scaled_action: [-0.14254844]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.4870762]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.7065583]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.06560695]\n","step\n","scaled_action: [-0.4941677]\n","step\n","scaled_action: [-0.6180543]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.05728859]\n","step\n","scaled_action: [0.66033316]\n","step\n","scaled_action: [0.82382137]\n","step\n","scaled_action: [-0.04956567]\n","step\n","scaled_action: [-0.3489802]\n","step\n","scaled_action: [0.00254953]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.16852903]\n","step\n","scaled_action: [-0.833548]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.3263688]\n","step\n","scaled_action: [-0.1430571]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.90268606]\n","step\n","scaled_action: [-0.34386343]\n","step\n","scaled_action: [0.02649117]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.37206602]\n","step\n","scaled_action: [-0.6245288]\n","step\n","scaled_action: [-0.44526184]\n","step\n","scaled_action: [-0.88237995]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.47741234]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.14614385]\n","step\n","scaled_action: [0.7021683]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.7161473]\n","step\n","scaled_action: [0.908385]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.4909693]\n","step\n","scaled_action: [0.49084973]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.83261967]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.05345786]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.8470186]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.38273337]\n","step\n","scaled_action: [-0.73170185]\n","step\n","scaled_action: [0.09473343]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.72524583]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.83769655]\n","step\n","scaled_action: [-0.16091463]\n","step\n","scaled_action: [0.5786897]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.24572931]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.6611004]\n","step\n","scaled_action: [0.09796067]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.41636798]\n","step\n","scaled_action: [0.42358524]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.4726498]\n","step\n","scaled_action: [0.597583]\n","step\n","scaled_action: [0.75988424]\n","step\n","scaled_action: [-0.32460725]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.17787436]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.9690052]\n","step\n","scaled_action: [0.4275901]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.4116896]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.58855414]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.2255733]\n","step\n","scaled_action: [0.28302708]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.42937246]\n","step\n","scaled_action: [0.04029286]\n","step\n","scaled_action: [0.09119573]\n","step\n","scaled_action: [-0.8220521]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.743962]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.08059026]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.3896963]\n","step\n","scaled_action: [-0.20714526]\n","step\n","scaled_action: [0.48875266]\n","step\n","scaled_action: [0.38537756]\n","step\n","scaled_action: [-0.49184746]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.75899917]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.21996039]\n","step\n","scaled_action: [0.6349385]\n","step\n","scaled_action: [0.22554705]\n","step\n","scaled_action: [0.20806235]\n","step\n","scaled_action: [-0.17139496]\n","step\n","scaled_action: [0.522362]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.22051284]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.52658796]\n","step\n","scaled_action: [0.58610773]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.4684461]\n","step\n","scaled_action: [0.25162354]\n","step\n","scaled_action: [-0.5064936]\n","step\n","scaled_action: [0.30133134]\n","step\n","scaled_action: [0.8691088]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.8578286]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.8155011]\n","step\n","scaled_action: [0.84884083]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.44044948]\n","step\n","scaled_action: [0.6879161]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.05723763]\n","step\n","scaled_action: [-0.5694963]\n","step\n","scaled_action: [0.5513538]\n","step\n","scaled_action: [-0.4495243]\n","step\n","scaled_action: [-0.08905943]\n","step\n","scaled_action: [-0.6163397]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.16915384]\n","step\n","scaled_action: [0.61949646]\n","step\n","scaled_action: [0.38267294]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.16204196]\n","step\n","scaled_action: [-0.48420212]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.51817834]\n","step\n","scaled_action: [-0.43676597]\n","step\n","scaled_action: [-0.720868]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.6125698]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.81678075]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.13179135]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.7137885]\n","step\n","scaled_action: [-0.9923284]\n","step\n","scaled_action: [-0.37274677]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.09172433]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.4881677]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.9751027]\n","step\n","scaled_action: [0.5315102]\n","step\n","scaled_action: [0.3584643]\n","step\n","scaled_action: [-0.52503544]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.700856]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.22289088]\n","step\n","scaled_action: [-0.7944089]\n","step\n","scaled_action: [-0.66664916]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.8380318]\n","step\n","scaled_action: [-0.63198954]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.80736077]\n","step\n","scaled_action: [0.97377944]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.27665544]\n","step\n","scaled_action: [-0.26943055]\n","step\n","scaled_action: [-0.93878365]\n","step\n","scaled_action: [-0.29158485]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.92161244]\n","step\n","scaled_action: [0.759053]\n","step\n","scaled_action: [0.9997536]\n","step\n","scaled_action: [0.99843836]\n","step\n","scaled_action: [-0.96504384]\n","step\n","scaled_action: [0.42498964]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.3571614]\n","step\n","scaled_action: [0.06004705]\n","step\n","scaled_action: [0.04696386]\n","step\n","scaled_action: [-0.41087443]\n","step\n","scaled_action: [0.8563641]\n","step\n","scaled_action: [-0.2511181]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.63627064]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.07016841]\n","step\n","scaled_action: [-0.5343727]\n","step\n","scaled_action: [-0.27501637]\n","step\n","scaled_action: [0.59768]\n","step\n","scaled_action: [0.16967309]\n","step\n","scaled_action: [-0.22425961]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.5928824]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.10172689]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.13615918]\n","step\n","scaled_action: [-0.20643842]\n","step\n","scaled_action: [-0.04279977]\n","step\n","scaled_action: [0.56269056]\n","step\n","scaled_action: [0.14012095]\n","step\n","scaled_action: [-0.45540282]\n","step\n","scaled_action: [0.06872229]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.17723781]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.00186794]\n","step\n","scaled_action: [0.82519865]\n","step\n","scaled_action: [-0.5406755]\n","step\n","scaled_action: [-0.17692813]\n","step\n","scaled_action: [-0.57651126]\n","step\n","scaled_action: [0.9575911]\n","step\n","scaled_action: [-0.97083753]\n","step\n","scaled_action: [0.1330849]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.9638689]\n","step\n","scaled_action: [-0.6658146]\n","step\n","scaled_action: [-0.8636644]\n","step\n","scaled_action: [-0.01395798]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.01318598]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.39208695]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.1307056]\n","step\n","scaled_action: [0.24000561]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.67930704]\n","step\n","scaled_action: [-0.5641914]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.24599703]\n","step\n","scaled_action: [0.80103433]\n","step\n","scaled_action: [-0.88408077]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.69619673]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.30018857]\n","step\n","scaled_action: [-0.7688402]\n","step\n","scaled_action: [0.5770678]\n","step\n","scaled_action: [-0.45529804]\n","step\n","scaled_action: [-0.54080945]\n","step\n","scaled_action: [0.40491116]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.30536282]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.57955]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.726102]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.56650984]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.48300385]\n","step\n","scaled_action: [-0.41372734]\n","step\n","scaled_action: [-0.01223418]\n","step\n","scaled_action: [0.2623367]\n","step\n","scaled_action: [-0.27495006]\n","step\n","scaled_action: [0.48983854]\n","step\n","scaled_action: [0.89950794]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.05852252]\n","step\n","scaled_action: [-0.7929546]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.28989807]\n","step\n","scaled_action: [0.33332753]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.80890214]\n","step\n","scaled_action: [-0.3597317]\n","step\n","scaled_action: [-0.9572302]\n","step\n","scaled_action: [-0.55998206]\n","step\n","scaled_action: [-0.8918438]\n","step\n","scaled_action: [0.73586106]\n","step\n","scaled_action: [-0.5422214]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.12850673]\n","step\n","scaled_action: [-0.75793207]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.8021635]\n","step\n","scaled_action: [-0.07285714]\n","step\n","scaled_action: [-0.2618106]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.09880465]\n","step\n","scaled_action: [0.82627654]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.7390107]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.6391742]\n","step\n","scaled_action: [-0.4548034]\n","step\n","scaled_action: [0.71643865]\n","step\n","scaled_action: [-0.4263698]\n","step\n","scaled_action: [-0.7908959]\n","step\n","scaled_action: [0.35087055]\n","step\n","scaled_action: [-0.7926558]\n","step\n","scaled_action: [0.01343919]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.61642146]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.448897]\n","step\n","scaled_action: [0.29639828]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.65844214]\n","step\n","scaled_action: [0.6214806]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.40835452]\n","step\n","scaled_action: [0.94285214]\n","step\n","scaled_action: [-0.5846362]\n","step\n","scaled_action: [0.9194798]\n","step\n","scaled_action: [-0.7525829]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.50117975]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.04678786]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.31583488]\n","step\n","scaled_action: [0.0951581]\n","step\n","scaled_action: [0.13685185]\n","step\n","scaled_action: [-0.7663269]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.3037175]\n","step\n","scaled_action: [-0.30806383]\n","step\n","scaled_action: [0.26652753]\n","step\n","scaled_action: [0.8127278]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.5233256]\n","step\n","scaled_action: [0.79369354]\n","step\n","scaled_action: [0.7313515]\n","step\n","scaled_action: [-0.43014857]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.419816]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.28857556]\n","step\n","scaled_action: [-0.8592206]\n","step\n","scaled_action: [0.6616896]\n","step\n","scaled_action: [-0.03568362]\n","step\n","scaled_action: [0.82831496]\n","step\n","scaled_action: [0.35715365]\n","step\n","scaled_action: [0.5794472]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.6041907]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.33298993]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.90739715]\n","step\n","scaled_action: [-0.22025375]\n","step\n","scaled_action: [0.69133496]\n","step\n","scaled_action: [-0.14943571]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.9391637]\n","step\n","scaled_action: [-0.31155172]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.50031275]\n","step\n","scaled_action: [0.01556998]\n","step\n","scaled_action: [-0.8139709]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.68839854]\n","step\n","scaled_action: [-0.965994]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.7999775]\n","step\n","scaled_action: [-0.59067374]\n","step\n","scaled_action: [0.59847903]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.0106452]\n","step\n","scaled_action: [-0.4362086]\n","step\n","scaled_action: [0.79924494]\n","step\n","scaled_action: [-0.28918827]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.34295154]\n","step\n","scaled_action: [0.66881573]\n","step\n","scaled_action: [-0.9401678]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.22030511]\n","step\n","scaled_action: [-0.64400107]\n","step\n","scaled_action: [-0.25560334]\n","step\n","scaled_action: [-0.7419648]\n","step\n","scaled_action: [0.80336666]\n","step\n","scaled_action: [-0.04149783]\n","step\n","scaled_action: [-0.43407866]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.5739826]\n","step\n","scaled_action: [-0.4063758]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.29474738]\n","step\n","scaled_action: [0.7464025]\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 200       |\n","|    ep_rew_mean        | -1.68e+03 |\n","| time/                 |           |\n","|    fps                | 327       |\n","|    iterations         | 100       |\n","|    time_elapsed       | 1         |\n","|    total_timesteps    | 500       |\n","| train/                |           |\n","|    entropy_loss       | -1.41     |\n","|    explained_variance | 0.00105   |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 99        |\n","|    policy_loss        | -32.9     |\n","|    std                | 0.993     |\n","|    value_loss         | 963       |\n","-------------------------------------\n","step\n","scaled_action: [0.37479225]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.73708844]\n","step\n","scaled_action: [-0.22665463]\n","step\n","scaled_action: [0.16868065]\n","step\n","scaled_action: [0.28447214]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.5239543]\n","step\n","scaled_action: [-0.7747659]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.0280254]\n","step\n","scaled_action: [0.70860046]\n","step\n","scaled_action: [-0.32515132]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.4947449]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.9280493]\n","step\n","scaled_action: [-0.49699467]\n","step\n","scaled_action: [-0.4211757]\n","step\n","scaled_action: [-0.20240408]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.03808308]\n","step\n","scaled_action: [-0.51966]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.98922294]\n","step\n","scaled_action: [0.91418755]\n","step\n","scaled_action: [0.5383053]\n","step\n","scaled_action: [-0.9829538]\n","step\n","scaled_action: [0.9512778]\n","step\n","scaled_action: [-0.12426195]\n","step\n","scaled_action: [-0.85813046]\n","step\n","scaled_action: [-0.65332454]\n","step\n","scaled_action: [-0.48513943]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.8860705]\n","step\n","scaled_action: [0.4145262]\n","step\n","scaled_action: [0.06416875]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.3378396]\n","step\n","scaled_action: [-0.8790302]\n","step\n","scaled_action: [-0.37884986]\n","step\n","scaled_action: [0.24044555]\n","step\n","scaled_action: [-0.38492662]\n","step\n","scaled_action: [-0.77994984]\n","step\n","scaled_action: [-0.43544644]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.7067492]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.5347494]\n","step\n","scaled_action: [-0.9941703]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.02518439]\n","step\n","scaled_action: [0.1517699]\n","step\n","scaled_action: [-0.7851633]\n","step\n","scaled_action: [-0.5124789]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.48834997]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.60693866]\n","step\n","scaled_action: [0.4074905]\n","step\n","scaled_action: [-0.7072435]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.2761085]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.88297504]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.03277111]\n","step\n","scaled_action: [-0.75027806]\n","step\n","scaled_action: [0.65301085]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.5785224]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.34370458]\n","step\n","scaled_action: [-0.11749239]\n","step\n","scaled_action: [0.53118587]\n","step\n","scaled_action: [-0.00514668]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.5130329]\n","step\n","scaled_action: [-0.4661113]\n","step\n","scaled_action: [0.08995709]\n","step\n","scaled_action: [0.554169]\n","step\n","scaled_action: [-0.13849606]\n","step\n","scaled_action: [0.4606507]\n","step\n","scaled_action: [-0.15368271]\n","step\n","scaled_action: [-0.21049944]\n","step\n","scaled_action: [-0.14089392]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.90573615]\n","step\n","scaled_action: [0.7806579]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.8426704]\n","step\n","scaled_action: [0.83669037]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.7069044]\n","step\n","scaled_action: [0.92809635]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.45727348]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.3242756]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.12518525]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.755979]\n","step\n","scaled_action: [0.44391364]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.05452019]\n","step\n","scaled_action: [0.9293097]\n","step\n","scaled_action: [0.4070776]\n","step\n","scaled_action: [-0.43783692]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.3225042]\n","step\n","scaled_action: [-0.9782098]\n","step\n","scaled_action: [0.08966681]\n","step\n","scaled_action: [-0.8284253]\n","step\n","scaled_action: [0.02970107]\n","step\n","scaled_action: [0.17828283]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.18161924]\n","step\n","scaled_action: [-0.8239799]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.00206923]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.72328055]\n","step\n","scaled_action: [-0.01123732]\n","step\n","scaled_action: [-0.55457693]\n","step\n","scaled_action: [0.44419992]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.71781796]\n","step\n","scaled_action: [0.27094537]\n","step\n","scaled_action: [-0.4680415]\n","step\n","scaled_action: [0.45270264]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.7424807]\n","step\n","scaled_action: [0.37402618]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.8562232]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.941468]\n","step\n","scaled_action: [0.5503588]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.0403333]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.2788295]\n","step\n","scaled_action: [-0.53247267]\n","step\n","scaled_action: [0.68751055]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.9527148]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.9251266]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.6584597]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.5722876]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.46412498]\n","step\n","scaled_action: [-0.31753823]\n","step\n","scaled_action: [-0.69839185]\n","step\n","scaled_action: [-0.8141886]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.61752325]\n","step\n","scaled_action: [0.46142846]\n","step\n","scaled_action: [-0.44833684]\n","step\n","scaled_action: [-0.48914117]\n","step\n","scaled_action: [0.28104055]\n","step\n","scaled_action: [-0.5744739]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.26172698]\n","step\n","scaled_action: [-0.50676745]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.1827631]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.11128736]\n","step\n","scaled_action: [-0.34264567]\n","step\n","scaled_action: [-0.08283839]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.01123232]\n","step\n","scaled_action: [-0.33169115]\n","step\n","scaled_action: [-0.9935173]\n","step\n","scaled_action: [0.35046244]\n","step\n","scaled_action: [-0.15268078]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.293602]\n","step\n","scaled_action: [0.27322698]\n","step\n","scaled_action: [0.07345396]\n","step\n","scaled_action: [0.6792393]\n","step\n","scaled_action: [-0.46772707]\n","step\n","scaled_action: [-0.4721814]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.41826943]\n","step\n","scaled_action: [0.01456997]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.42304116]\n","step\n","scaled_action: [-0.8091773]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.5896565]\n","step\n","scaled_action: [0.45089123]\n","step\n","scaled_action: [-0.29048294]\n","step\n","scaled_action: [-0.43538594]\n","step\n","scaled_action: [-0.97687817]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.613534]\n","step\n","scaled_action: [0.8628263]\n","step\n","scaled_action: [0.04161787]\n","step\n","scaled_action: [-0.616432]\n","step\n","scaled_action: [-0.9820001]\n","step\n","scaled_action: [-0.9864333]\n","step\n","scaled_action: [-0.80311763]\n","step\n","scaled_action: [0.07636565]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.7055707]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.7210052]\n","step\n","scaled_action: [-0.10951604]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.3365688]\n","step\n","scaled_action: [-0.55185086]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.22466259]\n","step\n","scaled_action: [-0.72963196]\n","step\n","scaled_action: [0.6584666]\n","step\n","scaled_action: [-0.32251707]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.9852736]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.23629642]\n","step\n","scaled_action: [0.23413086]\n","step\n","scaled_action: [0.6034573]\n","step\n","scaled_action: [-0.36995828]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.5263787]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.27738452]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.43387294]\n","step\n","scaled_action: [-0.5063673]\n","step\n","scaled_action: [0.00019526]\n","step\n","scaled_action: [0.3356819]\n","step\n","scaled_action: [0.53415155]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.4366749]\n","step\n","scaled_action: [0.04585786]\n","step\n","scaled_action: [0.83107805]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.28313768]\n","step\n","scaled_action: [-0.8201741]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.47660613]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.11514354]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.7685579]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.50554013]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.42625257]\n","step\n","scaled_action: [0.58358365]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.1393007]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.17123437]\n","step\n","scaled_action: [-0.2568307]\n","step\n","scaled_action: [0.60951626]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.5599055]\n","step\n","scaled_action: [-0.57302856]\n","step\n","scaled_action: [-0.3049217]\n","step\n","scaled_action: [0.7580277]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.1848867]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.13642111]\n","step\n","scaled_action: [-0.5944681]\n","step\n","scaled_action: [0.13397108]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.9412869]\n","step\n","scaled_action: [-0.57560325]\n","step\n","scaled_action: [-0.57007056]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.72347033]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.11934119]\n","step\n","scaled_action: [-0.20801166]\n","step\n","scaled_action: [0.01477408]\n","step\n","scaled_action: [-0.8880929]\n","step\n","scaled_action: [-0.88130474]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.79681754]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.03931817]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.5113349]\n","step\n","scaled_action: [-0.91688013]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.9727686]\n","step\n","scaled_action: [-0.9517572]\n","step\n","scaled_action: [0.01419868]\n","step\n","scaled_action: [-0.37684745]\n","step\n","scaled_action: [-0.64577305]\n","step\n","scaled_action: [-0.93938947]\n","step\n","scaled_action: [0.979265]\n","step\n","scaled_action: [0.774329]\n","step\n","scaled_action: [-0.84170055]\n","step\n","scaled_action: [-0.9487971]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.83817434]\n","step\n","scaled_action: [-0.62295854]\n","step\n","scaled_action: [-0.8934413]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.36673737]\n","step\n","scaled_action: [-0.5973138]\n","step\n","scaled_action: [-0.27192914]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.60950816]\n","step\n","scaled_action: [0.6769215]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.2651511]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.30594987]\n","step\n","scaled_action: [0.47338542]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.56084716]\n","step\n","scaled_action: [-0.8364214]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.10737026]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.52730936]\n","step\n","scaled_action: [-0.9057274]\n","step\n","scaled_action: [-0.06450066]\n","step\n","scaled_action: [-0.5536482]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.98152417]\n","step\n","scaled_action: [0.2774978]\n","step\n","scaled_action: [-0.3389947]\n","step\n","scaled_action: [-0.02467924]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.560913]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.32760656]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.68667173]\n","step\n","scaled_action: [0.3810091]\n","step\n","scaled_action: [0.00366461]\n","step\n","scaled_action: [-0.87500536]\n","step\n","scaled_action: [-0.8161002]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.68040884]\n","step\n","scaled_action: [-0.6080786]\n","step\n","scaled_action: [-0.03209496]\n","step\n","scaled_action: [0.53290904]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.2435305]\n","step\n","scaled_action: [-0.62800413]\n","step\n","scaled_action: [0.5099994]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.35749638]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.33731788]\n","step\n","scaled_action: [-0.7902806]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.40239674]\n","step\n","scaled_action: [0.16306901]\n","step\n","scaled_action: [0.11574185]\n","step\n","scaled_action: [-0.37826604]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.89604294]\n","step\n","scaled_action: [-0.45817205]\n","step\n","scaled_action: [-0.24565005]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.5003106]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.77746016]\n","step\n","scaled_action: [-0.64281535]\n","step\n","scaled_action: [0.08678228]\n","step\n","scaled_action: [-0.4098087]\n","step\n","scaled_action: [-0.94759333]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.0063224]\n","step\n","scaled_action: [0.30326194]\n","step\n","scaled_action: [-0.3542188]\n","step\n","scaled_action: [0.34895605]\n","step\n","scaled_action: [-0.86847186]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.7120572]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.85033023]\n","step\n","scaled_action: [0.17750597]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.3249327]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.46375436]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.92195165]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.59470475]\n","step\n","scaled_action: [-0.5187179]\n","step\n","scaled_action: [-0.20809355]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.5200034]\n","step\n","scaled_action: [0.97435236]\n","step\n","scaled_action: [-0.8531436]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.17401583]\n","step\n","scaled_action: [0.29800615]\n","step\n","scaled_action: [0.39981532]\n","step\n","scaled_action: [0.05946627]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.04216328]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.00082792]\n","step\n","scaled_action: [-0.8838327]\n","step\n","scaled_action: [0.9259809]\n","step\n","scaled_action: [-0.7219224]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [-0.14661013]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.10229249]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.9389464]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [-0.8268819]\n","step\n","scaled_action: [-0.4217179]\n","step\n","scaled_action: [0.00336967]\n","step\n","scaled_action: [1.]\n","step\n","scaled_action: [0.22015566]\n","step\n","scaled_action: [0.26723862]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.10767591]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.64387596]\n","step\n","scaled_action: [-1.]\n","step\n","scaled_action: [0.82243925]\n","step\n","scaled_action: [-0.5992793]\n","step\n","scaled_action: [-1.]\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 200       |\n","|    ep_rew_mean        | -1.76e+03 |\n","| time/                 |           |\n","|    fps                | 323       |\n","|    iterations         | 200       |\n","|    time_elapsed       | 3         |\n","|    total_timesteps    | 1000      |\n","| train/                |           |\n","|    entropy_loss       | -1.41     |\n","|    explained_variance | -0.00213  |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 199       |\n","|    policy_loss        | -30.4     |\n","|    std                | 0.996     |\n","|    value_loss         | 771       |\n","-------------------------------------\n"]}],"source":["model_2 = A2C(\"MlpPolicy\", normalized_env, verbose=1).learn(int(1000))"]},{"cell_type":"markdown","metadata":{"id":"5BxqXd_6dpJx"},"source":["## Additional wrappers: VecEnvWrappers\n","\n","In the same vein as gym wrappers, stable baselines provide wrappers for `VecEnv`. Among the different wrappers that exist (and you can create your own), you should know:\n","\n","- VecNormalize: it computes a running mean and standard deviation to normalize observation and returns\n","- VecFrameStack: it stacks several consecutive observations (useful to integrate time in the observation, e.g. successive frame of an atari game)\n","\n","More info in the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#wrappers)\n","\n","Note: when using `VecNormalize` wrapper, you must save the running mean and std along with the model, otherwise you will not get proper results when loading the agent again. If you use the [rl zoo](https://github.com/DLR-RM/rl-baselines3-zoo), this is done automatically"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"zuIcbfv3g9dd","executionInfo":{"status":"ok","timestamp":1688975009505,"user_tz":-480,"elapsed":373,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}}},"outputs":[],"source":["from stable_baselines3.common.vec_env import VecNormalize, VecFrameStack\n","\n","env = DummyVecEnv([lambda: gym.make(\"Pendulum-v1\")])\n","normalized_vec_env = VecNormalize(env)"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"-PAbu21pg90A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688975024846,"user_tz":-480,"elapsed":408,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}},"outputId":"96ed3c8a-92bb-42ed-ec76-e4cb60b0aa9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.21886215  0.96008146 -0.98896295]] [-10.]\n","[[-0.3341619  1.2439088 -1.2707156]] [-2.0191226]\n","[[-0.1861328   1.3454331  -0.52893656]] [-1.2671015]\n","[[ 0.430721   1.4705782 -1.5466713]] [-0.90647316]\n","[[ 1.4376166  1.5686135 -1.713345 ]] [-0.70060086]\n","[[ 1.9269917  1.590302  -1.130473 ]] [-0.56578356]\n","[[1.9748877 1.533618  0.4115685]] [-0.46950403]\n","[[1.8665247 1.4359976 1.6901478]] [-0.4000109]\n","[[1.6791254 1.3114206 2.1268141]] [-0.35169232]\n","[[1.4451702 1.170348  2.1323037]] [-0.3181389]\n"]}],"source":["obs = normalized_vec_env.reset()\n","for _ in range(10):\n","    action = [normalized_vec_env.action_space.sample()]\n","    obs, reward, _, _ = normalized_vec_env.step(action)\n","    print(obs, reward)"]},{"cell_type":"markdown","metadata":{"id":"UEpTys28Wz05"},"source":["## Exercise: code you own monitor wrapper\n","\n","Now that you know how does a wrapper work and what you can do with it, it's time to experiment.\n","\n","The goal here is to create a wrapper that will monitor the training progress, storing both the episode reward (sum of reward for one episode) and episode length (number of steps in for the last episode).\n","\n","You will return those values using the `info` dict after each end of episode."]},{"cell_type":"code","execution_count":33,"metadata":{"id":"8FWeDRd5W7hO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688975457203,"user_tz":-480,"elapsed":424,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}},"outputId":"2280ed22-b213-4a39-974c-6fea9b43db63"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["class MyMonitorWrapper(gym.Wrapper):\n","    \"\"\"\n","    :param env: (gym.Env) Gym environment that will be wrapped\n","    \"\"\"\n","\n","    def __init__(self, env):\n","        # Call the parent constructor, so we can access self.env later\n","        super().__init__(env)\n","        # === YOUR CODE HERE ===#\n","        # Initialize the variables that will be used\n","        # to store the episode length and episode reward\n","        self.epi_reward = 0\n","        self.epi_len = 0\n","        # ====================== #\n","\n","    def reset(self, **kwargs):\n","        \"\"\"\n","        Reset the environment\n","        \"\"\"\n","        obs = self.env.reset(**kwargs)\n","        # === YOUR CODE HERE ===#\n","        # Reset the variables\n","        self.epi_reward = 0\n","        self.epi_len = 0\n","        # ====================== #\n","        return obs\n","\n","    def step(self, action):\n","        \"\"\"\n","        :param action: ([float] or int) Action taken by the agent\n","        :return: (np.ndarray, float, bool, bool, dict)\n","            observation, reward, is the episode over?, is the episode truncated?, additional information\n","        \"\"\"\n","        obs, reward, terminated, truncated, info = self.env.step(action)\n","        # === YOUR CODE HERE ===#\n","        # Update the current episode reward and episode length\n","        self.epi_reward = reward\n","        self.epi_len += 1\n","        # ====================== #\n","\n","        if terminated or truncated:\n","            # === YOUR CODE HERE ===#\n","            # Store the episode length and episode reward in the info dict\n","            info[\"epi_reward\"] = self.epi_reward\n","            info[\"epi_length\"] = self.epi_len\n","\n","            # ====================== #\n","        return obs, reward, terminated, truncated, info"]},{"cell_type":"markdown","metadata":{"id":"d4fY4QwWXNFK"},"source":["#### Test your wrapper"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"bJbUG-A_liYt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688975510808,"user_tz":-480,"elapsed":51374,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}},"outputId":"127340d6-bea8-475d-f868-4c1422dc6236"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting box2d-py\n","  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/374.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m368.6/374.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp310-cp310-linux_x86_64.whl size=2811883 sha256=84fdcfa6cc0813e0a0cd0e080f85b749c8c1c4015d588e15544d4c0bff713230\n","  Stored in directory: /root/.cache/pip/wheels/47/01/d2/6a780da77ccb98b1d2facdd520a8d10838a03b590f6f8d50c0\n","Successfully built box2d-py\n","Installing collected packages: box2d-py\n","Successfully installed box2d-py-2.3.8\n"]}],"source":["# To use LunarLander, you need to install box2d box2d-kengz (pip) and swig (apt-get)\n","!pip install box2d-py"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"oWZp1olSXMUg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688975815920,"user_tz":-480,"elapsed":434,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}},"outputId":"5a833c7d-a248-4f29-a2a7-9cd26166b4e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["action: 1\n","info: {}\n","action: 1\n","info: {}\n","action: 2\n","info: {}\n","action: 2\n","info: {}\n","action: 3\n","info: {}\n","action: 1\n","info: {}\n","action: 2\n","info: {}\n","action: 2\n","info: {}\n","action: 0\n","info: {}\n","action: 2\n","info: {}\n","action: 0\n","info: {}\n","action: 2\n","info: {}\n","action: 3\n","info: {}\n","action: 3\n","info: {}\n","action: 0\n","info: {}\n","action: 2\n","info: {}\n","action: 2\n","info: {}\n","action: 2\n","info: {}\n","action: 3\n","info: {}\n","action: 3\n","info: {}\n","action: 1\n","info: {}\n","action: 1\n","info: {}\n","action: 3\n","info: {}\n","action: 1\n","info: {}\n","action: 3\n","info: {}\n","action: 3\n","info: {}\n","action: 3\n","info: {}\n","action: 1\n","info: {}\n","action: 3\n","info: {}\n","action: 1\n","info: {}\n","action: 2\n","info: {}\n","action: 0\n","info: {}\n","action: 0\n","info: {}\n","action: 3\n","info: {}\n","action: 1\n","info: {}\n","action: 1\n","info: {}\n","action: 1\n","info: {}\n","action: 0\n","info: {}\n","action: 0\n","info: {}\n","action: 2\n","info: {}\n","action: 3\n","info: {}\n","action: 0\n","info: {}\n","action: 2\n","info: {}\n","action: 1\n","info: {}\n","action: 2\n","info: {}\n","action: 2\n","info: {}\n","action: 0\n","info: {}\n","action: 2\n","info: {}\n","action: 0\n","info: {}\n","action: 2\n","info: {}\n","action: 0\n","info: {}\n","action: 2\n","info: {}\n","action: 3\n","info: {}\n","action: 1\n","info: {}\n","action: 3\n","info: {}\n","action: 1\n","info: {}\n","action: 2\n","info: {}\n","action: 3\n","info: {}\n","action: 1\n","info: {}\n","action: 0\n","info: {}\n","action: 3\n","info: {}\n","action: 0\n","info: {}\n","action: 1\n","info: {}\n","action: 2\n","info: {}\n","action: 3\n","info: {}\n","action: 3\n","info: {}\n","action: 1\n","info: {}\n","action: 2\n","info: {}\n","action: 3\n","info: {}\n","action: 3\n","info: {}\n","action: 0\n","info: {}\n","action: 1\n","info: {}\n","action: 2\n","info: {}\n","action: 0\n","info: {}\n","action: 2\n","info: {}\n","action: 3\n","info: {}\n","action: 2\n","info: {}\n","action: 0\n","info: {}\n","action: 0\n","info: {}\n","action: 1\n","info: {}\n","action: 1\n","info: {}\n","action: 2\n","info: {}\n","action: 3\n","info: {}\n","action: 3\n","info: {}\n","action: 0\n","info: {}\n","action: 0\n","info: {}\n","action: 3\n","info: {}\n","action: 2\n","info: {}\n","action: 1\n","info: {}\n","action: 3\n","info: {}\n","action: 0\n","info: {}\n","action: 2\n","info: {}\n","action: 2\n","info: {}\n","action: 1\n","info: {}\n","action: 1\n","info: {}\n","action: 2\n","info: {}\n","action: 1\n","info: {}\n","action: 1\n","info: {}\n","action: 1\n","info: {}\n","action: 0\n","info: {}\n","action: 3\n","info: {}\n","action: 2\n","info: {}\n","action: 0\n","info: {}\n","action: 2\n","info: {}\n","action: 2\n","info: {}\n","action: 2\n","info: {}\n","action: 3\n","info: {'epi_reward': -100, 'epi_length': 108}\n"]}],"source":["env = gym.make(\"LunarLander-v2\")\n","# === YOUR CODE HERE ===#\n","# Wrap the environment\n","env = MyMonitorWrapper(env)\n","# Reset the environment\n","obs = env.reset()\n","# Take random actions in the environment and check\n","# that it returns the correct values after the end of each episode\n","obs, reward, terminated, truncated, info = env.step(action)\n","while not (terminated or truncated):\n","  action = env.action_space.sample()\n","  print(f\"action: {action}\")\n","  obs, reward, terminated, truncated, info = env.step(action)\n","  print(f\"info: {info}\")\n","# ====================== #"]},{"cell_type":"markdown","metadata":{"id":"dJ2IqSM2eOt8"},"source":[" # Conclusion\n","\n"," In this notebook, we have seen:\n"," - how to easily save and load a model\n"," - what is wrapper and what we can do with it\n"," - how to create your own wrapper"]},{"cell_type":"markdown","metadata":{"id":"qhWB_bHpSkas"},"source":["## Wrapper Bonus: changing the observation space: a wrapper for episode of fixed length"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"bBlS9YxYSpJn","executionInfo":{"status":"ok","timestamp":1688975828180,"user_tz":-480,"elapsed":395,"user":{"displayName":"Valentine Kevin","userId":"09325380893251256195"}}},"outputs":[],"source":["from gym.wrappers import TimeLimit\n","\n","\n","class TimeFeatureWrapper(gym.Wrapper):\n","    \"\"\"\n","    Add remaining time to observation space for fixed length episodes.\n","    See https://arxiv.org/abs/1712.00378 and https://github.com/aravindr93/mjrl/issues/13.\n","\n","    :param env: (gym.Env)\n","    :param max_steps: (int) Max number of steps of an episode\n","        if it is not wrapped in a TimeLimit object.\n","    :param test_mode: (bool) In test mode, the time feature is constant,\n","        equal to zero. This allow to check that the agent did not overfit this feature,\n","        learning a deterministic pre-defined sequence of actions.\n","    \"\"\"\n","\n","    def __init__(self, env, max_steps=1000, test_mode=False):\n","        assert isinstance(env.observation_space, gym.spaces.Box)\n","        # Add a time feature to the observation\n","        low, high = env.observation_space.low, env.observation_space.high\n","        low, high = np.concatenate((low, [0])), np.concatenate((high, [1.0]))\n","        env.observation_space = gym.spaces.Box(low=low, high=high, dtype=np.float32)\n","\n","        super().__init__(env)\n","\n","        if isinstance(env, TimeLimit):\n","            self._max_steps = env._max_episode_steps\n","        else:\n","            self._max_steps = max_steps\n","        self._current_step = 0\n","        self._test_mode = test_mode\n","\n","    def reset(self, **kwargs):\n","        self._current_step = 0\n","        obs, info = self.env.reset(**kwargs)\n","        return self._get_obs(obs), info\n","\n","    def step(self, action):\n","        self._current_step += 1\n","        obs, reward, terminated, truncated, info = self.env.step(action)\n","        return self._get_obs(obs), reward, terminated, truncated, info\n","\n","    def _get_obs(self, obs):\n","        \"\"\"\n","        Concatenate the time feature to the current observation.\n","\n","        :param obs: (np.ndarray)\n","        :return: (np.ndarray)\n","        \"\"\"\n","        # Remaining time is more general\n","        time_feature = 1 - (self._current_step / self._max_steps)\n","        if self._test_mode:\n","            time_feature = 1.0\n","        # Optionally: concatenate [time_feature, time_feature ** 2]\n","        return np.concatenate((obs, [time_feature]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z-vWgkZzd4F1"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Ojn4nvNNRUoT"},"source":["## Going further - Saving format\n","\n","The format for saving and loading models is a zip-archived JSON dump and NumPy zip archive of the arrays:\n","```\n","saved_model.zip/\n","├── data              JSON file of class-parameters (dictionary)\n","├── parameter_list    JSON file of model parameters and their ordering (list)\n","├── parameters        Bytes from numpy.savez (a zip file of the numpy arrays). ...\n","    ├── ...           Being a zip-archive itself, this object can also be opened ...\n","        ├── ...       as a zip-archive and browsed.\n","```"]},{"cell_type":"markdown","metadata":{"id":"QWAcc8RFRUoU"},"source":["## Save and find"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4tcQxzSCRUoV"},"outputs":[],"source":["# Create save dir\n","save_dir = \"/tmp/gym/\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","model = PPO(\"MlpPolicy\", \"Pendulum-v1\", verbose=0).learn(8000)\n","model.save(save_dir + \"/PPO_tutorial\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rGaMNz4HRUoX"},"outputs":[],"source":["!ls /tmp/gym/PPO_tutorial*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gYY3nQyyRUoa"},"outputs":[],"source":["import zipfile\n","\n","archive = zipfile.ZipFile(\"/tmp/gym/PPO_tutorial.zip\", \"r\")\n","for f in archive.filelist:\n","    print(f.filename)"]},{"cell_type":"markdown","metadata":{"id":"cPKkkTvjRUo2"},"source":["## Exporting saved models\n","\n","And finally some further reading for those who want to export to tensorflowJS or Java.\n","\n","https://stable-baselines3.readthedocs.io/en/master/guide/export.html"]},{"cell_type":"markdown","metadata":{"id":"BSwjy6ccbyOe"},"source":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}