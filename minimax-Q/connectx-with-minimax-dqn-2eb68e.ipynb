{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConnectX with Minimax-DQN\n",
    "In this notebook I show a simple extension of [DQN](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) to 2 player alternating-turn games known as Minimax-DQN. [Minimax-DQN](https://arxiv.org/abs/1901.00137) is based on [Minimax Q learning](https://www2.cs.duke.edu/courses/spring07/cps296.3/littman94markov.pdf) a paper from 1994. Minimax-DQN is the modern neural network equivalent to the tabular Q learning version.\n",
    "\n",
    "## DQN\n",
    "Vanilla DQN isn't meant for the Multi-Agent Reinforcement Learning (MARL) setting, but we can use DQN here to learn against a fixed opponent and consider it as part of the environment. For example in [this notebook](https://www.kaggle.com/phunghieu/connectx-with-deep-q-learning/notebook), off of which my notebook was forked, DQN is used to learn the game by playing against a random opponent. Note that a random opponent is considered \"fixed\" in the sense that its policy does not change over time.\n",
    "\n",
    "## IQL\n",
    "To learn in self play, one *can* simply pit 2 DQN agents against each other in the environment but this violates the fundamental assumption of Q learning that the environment is fixed. This is referred to as Independent Q learning (IQL) in this [paper](https://ora.ox.ac.uk/objects/uuid:2b650b3b-2fce-4875-b4df-70f4a4d64c8a/download_file?file_format=pdf&safe_filename=main.pdf&type_of_work=Conference+item) which discussed a way of making this technique work better in practice.\n",
    "\n",
    "## Minimax DQN\n",
    "It is however possible to do a more principled extension of DQN. In DQN we have the bellman equation \n",
    "\n",
    "$Q(s,p)=r+\\gamma max_a Q(s',a)$\n",
    "\n",
    "with $s$ the current state, $s'$ the next state, $p$ the action that lead from $s$ to $s'$, $r$ the immediate reward of going from state $s$ to $s'$ due to action $p$ and $\\gamma$ the discount factor of DQN.\n",
    "\n",
    "For a 1v1 alternating-turn game we can formalise our problem as a zero-sum game, where 1 is a certain win, -1 is a certain loss and the bellman equation becomes\n",
    "\n",
    "$Q(s,p,player\\_0)=r-\\gamma max_a Q(s',a,player\\_1)$\n",
    "\n",
    "Note that this equation is very much akin to what is done in [Negamax](https://en.wikipedia.org/wiki/Negamax#Negamax_base_algorithm).\n",
    "\n",
    "## \"Nash-DQN\"\n",
    "For your curiosity, Minimax-DQN can be generalised to 1v1 simultaneous-turn games by having the network output a matrix of Q values of size \\[N_actions,N_Actions\\] representing the value of each possible pairs of actions for both players. This can then be viewed as the payoff matrix of the game which can be solved with algorithms [such as this one](http://code.activestate.com/recipes/496825-game-theory-payoff-matrix-solver/). This again allows to transform the game back into a single agent form. More details in [this article](https://github.com/pb4git/Nash-DQN-CSB-Article) where me and a friend used this technique on another game.\n",
    "\n",
    "## Further improvements\n",
    "* The DQN network learned in this manner could be wrapped in a Negamax search, which would, in my experience, yield massive elo gains. The deep Q network can be used as the evaluation function required in a Negamax agent.\n",
    "* DQN can be improved with [Double DQN (DDQN)](https://arxiv.org/abs/1509.06461), [Prioritised Experience Replay](https://arxiv.org/abs/1511.05952), [Duelling DQN](https://arxiv.org/abs/1511.06581), [Noisy DQN](https://arxiv.org/abs/1706.10295) and the other improvements detailed in [the Rainbow paper](https://arxiv.org/abs/1710.02298).\n",
    "\n",
    "This agent currently has a leaderboard score of ~1050 playing greedily on the Q values. The current \\#1 player [is also using this technique](https://www.kaggle.com/c/connectx/discussion/129145)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-04T09:38:45.213291Z",
     "iopub.status.busy": "2023-11-04T09:38:45.212972Z",
     "iopub.status.idle": "2023-11-04T09:38:48.482303Z",
     "shell.execute_reply": "2023-11-04T09:38:48.481492Z",
     "shell.execute_reply.started": "2023-11-04T09:38:45.213243Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import os \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from kaggle_environments import evaluate, make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2023-11-04T09:38:48.485356Z",
     "iopub.status.busy": "2023-11-04T09:38:48.484855Z",
     "iopub.status.idle": "2023-11-04T09:38:48.493668Z",
     "shell.execute_reply": "2023-11-04T09:38:48.492574Z",
     "shell.execute_reply.started": "2023-11-04T09:38:48.485090Z"
    }
   },
   "outputs": [],
   "source": [
    "Train = True\n",
    "gamma = 0.99\n",
    "copy_step = 250\n",
    "hidden_units = [100, 200, 200, 100]\n",
    "max_experiences = 1000000\n",
    "min_experiences = 100000\n",
    "Steps_Till_Backprop = 64\n",
    "batch_size = 512\n",
    "lr = 1e-3\n",
    "epsilon = 0.99\n",
    "decay = 0.9999\n",
    "min_epsilon = 0.1\n",
    "episodes = 20000 #Set this to longer\n",
    "precision = 5\n",
    "Discard_Q_Value = -1e7\n",
    "Metric_Titles = ['Max_Q','Avg_Q','Min_Q']\n",
    "N_Downsampling_Episodes = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI gym environment definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T09:38:48.496195Z",
     "iopub.status.busy": "2023-11-04T09:38:48.495792Z",
     "iopub.status.idle": "2023-11-04T09:38:48.512204Z",
     "shell.execute_reply": "2023-11-04T09:38:48.511332Z",
     "shell.execute_reply.started": "2023-11-04T09:38:48.496122Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConnectX(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.env = make('connectx', debug=False)\n",
    "\n",
    "        # Define required gym fields (examples):\n",
    "        config = self.env.configuration\n",
    "        self.action_space = gym.spaces.Discrete(config.columns)\n",
    "        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n",
    "\n",
    "    def step(self, actions):\n",
    "        return self.env.step(actions)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "    \n",
    "    def render(self, **kwargs):\n",
    "        return self.env.render(**kwargs)\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.env.state\n",
    "\n",
    "    def game_over(self):\n",
    "        return self.env.done\n",
    "\n",
    "    def current_player(self):\n",
    "        active = -1\n",
    "        if self.env.state[0].status == \"ACTIVE\":\n",
    "            active=0\n",
    "        if self.env.state[1].status == \"ACTIVE\":\n",
    "            active=1\n",
    "        return active\n",
    "\n",
    "    def get_configuration(self):\n",
    "        return self.env.configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T09:38:48.513746Z",
     "iopub.status.busy": "2023-11-04T09:38:48.513446Z",
     "iopub.status.idle": "2023-11-04T09:38:48.525129Z",
     "shell.execute_reply": "2023-11-04T09:38:48.524303Z",
     "shell.execute_reply.started": "2023-11-04T09:38:48.513696Z"
    }
   },
   "outputs": [],
   "source": [
    "class DeepModel(tf.keras.Model):\n",
    "    def __init__(self, num_states, hidden_units, num_actions):\n",
    "        super(DeepModel, self).__init__()\n",
    "        self.input_layer = tf.keras.layers.InputLayer(input_shape=(num_states,))\n",
    "        self.hidden_layers = []\n",
    "        for i in hidden_units:\n",
    "            self.hidden_layers.append(tf.keras.layers.Dense(i, activation='relu', kernel_initializer='he_normal'))\n",
    "        self.output_layer = tf.keras.layers.Dense(num_actions, activation='tanh', kernel_initializer='RandomNormal')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        z = self.input_layer(inputs)\n",
    "        for layer in self.hidden_layers:\n",
    "            z = layer(z)\n",
    "        output = self.output_layer(z)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T09:38:48.526917Z",
     "iopub.status.busy": "2023-11-04T09:38:48.526593Z",
     "iopub.status.idle": "2023-11-04T09:38:48.824672Z",
     "shell.execute_reply": "2023-11-04T09:38:48.823926Z",
     "shell.execute_reply.started": "2023-11-04T09:38:48.526847Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DeepModel(nn.Module):\n",
    "    def __init__(self, num_states, hidden_units, num_actions):\n",
    "        super(DeepModel, self).__init__()\n",
    "        self.input_layer = nn.Linear(num_states, hidden_units[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_units) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_units[i], hidden_units[i+1]))\n",
    "        self.output_layer = nn.Linear(hidden_units[-1], num_actions)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        z = self.input_layer(inputs)\n",
    "        z = torch.relu(z)\n",
    "        for layer in self.hidden_layers:\n",
    "            z = layer(z)\n",
    "            z = torch.relu(z)\n",
    "        output = self.output_layer(z)\n",
    "        output = torch.tanh(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T09:38:48.827272Z",
     "iopub.status.busy": "2023-11-04T09:38:48.826973Z",
     "iopub.status.idle": "2023-11-04T09:38:48.845605Z",
     "shell.execute_reply": "2023-11-04T09:38:48.844829Z",
     "shell.execute_reply.started": "2023-11-04T09:38:48.827198Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6, 15, 24])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "t1.sum(dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the tanh output layer which restricts output to \\[-1,1\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T09:38:50.344277Z",
     "iopub.status.busy": "2023-11-04T09:38:50.343917Z",
     "iopub.status.idle": "2023-11-04T09:38:50.387013Z",
     "shell.execute_reply": "2023-11-04T09:38:50.386198Z",
     "shell.execute_reply.started": "2023-11-04T09:38:50.344226Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr):\n",
    "        self.num_actions = num_actions\n",
    "        self.batch_size = batch_size\n",
    "#         self.optimizer = tf.keras.optimizers.Nadam(lr)\n",
    "        self.gamma = gamma\n",
    "        self.model = DeepModel(num_states, hidden_units, num_actions)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.experience = {'inputs': [], 'a': [], 'r': [], 'inputs2': [], 'done': []} # The buffer\n",
    "        self.max_experiences = max_experiences\n",
    "        self.min_experiences = min_experiences\n",
    "\n",
    "    def predict(self, inputs):\n",
    "#         return self.model(np.atleast_2d(inputs.astype('float32')))\n",
    "        return self.model(inputs)\n",
    "\n",
    "    #@tf.function\n",
    "    def train(self, TargetNet):\n",
    "        # Only start the training process when we have enough experiences in the buffer\n",
    "        if len(self.experience['inputs']) < self.min_experiences:\n",
    "            return 0\n",
    "\n",
    "        # Randomly select n experience in the buffer, n is batch-size\n",
    "        ids = np.random.randint(low=0, high=len(self.experience['inputs']), size=self.batch_size)\n",
    "        states = np.asarray([self.experience['inputs'][i] for i in ids])\n",
    "        actions = np.asarray([self.experience['a'][i] for i in ids])\n",
    "        rewards = np.asarray([self.experience['r'][i] for i in ids])\n",
    "\n",
    "        # Prepare labels for training process\n",
    "        states_next = np.asarray([self.experience['inputs2'][i] for i in ids])\n",
    "        dones = np.asarray([self.experience['done'][i] for i in ids])\n",
    "        \n",
    "        states = torch.tensor(states, dtype = torch.float32)\n",
    "        actions = torch.tensor(actions, dtype = torch.long) \n",
    "        rewards = torch.tensor(rewards, dtype = torch.float32)\n",
    "        states_next = torch.tensor(states_next, dtype = torch.float32)\n",
    "#         dones = torch.tensor(dones, dtype = torch.float32)\n",
    "        \n",
    "        # Find the value of the next states by computing the max over valid actions in these next states\n",
    "        # states_next[:,:self.num_actions]为什么只选前num_actions个位置的状态？因为这些位置如果已满，表示对于connectx该游戏来说，就不能在落子了！！！\n",
    "        Move_Validity = states_next[:,:self.num_actions] == 0\n",
    "        Next_Q_Values = TargetNet.predict(states_next)\n",
    "        Next_Q_Values = np.where(Move_Validity.detach().numpy(), Next_Q_Values.detach().numpy(), Discard_Q_Value)\n",
    "        value_next = -np.max(Next_Q_Values, axis=1)\n",
    "        \n",
    "        actual_values = np.where(dones, rewards.detach().numpy(), rewards.detach().numpy() + self.gamma * value_next)\n",
    "        actual_values = torch.tensor(actual_values, dtype = torch.float32)\n",
    "        \n",
    "#         with tf.GradientTape() as tape:\n",
    "#             selected_action_values = tf.math.reduce_sum(self.predict(states) * tf.one_hot(actions, self.num_actions), axis=1)\n",
    "#             loss = tf.math.reduce_sum(tf.square(actual_values - selected_action_values))\n",
    "#         variables = self.model.trainable_variables\n",
    "#         gradients = tape.gradient(loss, variables)\n",
    "#         self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        selected_action_values = (self.predict(states) * torch.nn.functional.one_hot(actions, self.num_actions)).sum(dim=1)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = ((actual_values - selected_action_values) ** 2).sum()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    # Get an action by using epsilon-greedy\n",
    "    def get_action(self, state, epsilon):\n",
    "        state_tensor = torch.tensor(np.atleast_2d(self.preprocess(state)), dtype = torch.float32)\n",
    "        prediction = self.predict(state_tensor)[0].detach().numpy()\n",
    "        if np.random.random() < epsilon:\n",
    "#             print(\"hoho: random action!\")\n",
    "            return int(np.random.choice([c for c in range(self.num_actions) if state.board[c] == 0])), prediction\n",
    "        else:\n",
    "#             print(\"hoho: max action!\")\n",
    "            for i in range(self.num_actions):\n",
    "                if state.board[i] != 0:\n",
    "                    prediction[i] = Discard_Q_Value\n",
    "            return int(np.argmax(prediction)) , prediction\n",
    "\n",
    "    def add_experience(self, exp):\n",
    "        if len(self.experience['inputs']) >= self.max_experiences:\n",
    "            for key in self.experience.keys():\n",
    "                self.experience[key].pop(0)\n",
    "        for key, value in exp.items():\n",
    "            self.experience[key].append(value)\n",
    "\n",
    "    def copy_weights(self, TrainNet):\n",
    "#         variables1 = self.model.trainable_variables\n",
    "#         variables2 = TrainNet.model.trainable_variables\n",
    "#         for v1, v2 in zip(variables1, variables2):\n",
    "#             v1.assign(v2.numpy())\n",
    "            \n",
    "        for param1, param2 in zip(self.model.parameters(), TrainNet.model.parameters()):\n",
    "            param1.data.copy_(param2.data)\n",
    "\n",
    "    def save_weights(self, path):\n",
    "#         self.model.save_weights(path)\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load_weights(self, path):\n",
    "#         ref_model = tf.keras.Sequential()\n",
    "\n",
    "#         ref_model.add(self.model.input_layer)\n",
    "#         for layer in self.model.hidden_layers:\n",
    "#             ref_model.add(layer)\n",
    "#         ref_model.add(self.model.output_layer)\n",
    "\n",
    "#         ref_model.load_weights(path)\n",
    "        \n",
    "        state_dict = torch.load(path)\n",
    "        self.model.load_state_dict(state_dict)\n",
    "    \n",
    "    # Each state is represented as 1s for the current player's pieces, -1s for the opponent's pieces and 0s\n",
    "    def preprocess(self, state):\n",
    "        return np.array([1 if val==state.mark else 0 if val==0 else -1 for val in state.board])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T09:38:50.471602Z",
     "iopub.status.busy": "2023-11-04T09:38:50.471269Z",
     "iopub.status.idle": "2023-11-04T09:38:50.477634Z",
     "shell.execute_reply": "2023-11-04T09:38:50.476549Z",
     "shell.execute_reply.started": "2023-11-04T09:38:50.471554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "t = [1, 2, 3, 4, 5]\n",
    "print([v for v in t if v > 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that in the DQN::preprocess() method we represent the state for the current player. The current player (given by state.mark==1/2) will see his pieces as 1s and the opponent's pieces as -1s. Once he plays, the next player will see the state \"from his point of view\" where all *his* pieces are 1s. That way the Neural network does not make a distinction between playing player 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that will play 1 ConnectX game and learn from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T09:38:51.653435Z",
     "iopub.status.busy": "2023-11-04T09:38:51.652889Z",
     "iopub.status.idle": "2023-11-04T09:38:51.674473Z",
     "shell.execute_reply": "2023-11-04T09:38:51.673396Z",
     "shell.execute_reply.started": "2023-11-04T09:38:51.653237Z"
    }
   },
   "outputs": [],
   "source": [
    "def play_game(env, TrainNet, TargetNet, epsilon, copy_step, Global_Step_Counter):\n",
    "    turns = 0\n",
    "    env.reset()\n",
    "    Metric_Buffer={key:[] for key in Metric_Titles}\n",
    "    while not env.game_over():\n",
    "        active = env.current_player()\n",
    "\n",
    "        # Using epsilon-greedy to get an action\n",
    "        observations = env.get_state()[active].observation\n",
    "        action, Q_Values = TrainNet.get_action(observations, epsilon)\n",
    "        Q_Values = [val for val in Q_Values if val!=Discard_Q_Value]\n",
    "        Metric_Buffer['Avg_Q'].append(np.mean(Q_Values))\n",
    "        Metric_Buffer['Max_Q'].append(np.max(Q_Values))\n",
    "        Metric_Buffer['Min_Q'].append(np.min(Q_Values))\n",
    "\n",
    "        # Caching the information of current state\n",
    "        prev_observations = observations\n",
    "\n",
    "        # Take action\n",
    "        env.step([action if i==active else None for i in [0,1]])\n",
    "\n",
    "        reward=env.get_state()[active].reward\n",
    "\n",
    "        #Convert environment's [0,0.5,1] reward scheme to [-1,1]\n",
    "        if env.game_over():\n",
    "            if reward == 1: # Won\n",
    "                reward = 1\n",
    "            elif reward == 0: # Lost\n",
    "                reward = -1\n",
    "            else: # Draw\n",
    "                reward = 0\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        next_active = 1 if active==0 else 0\n",
    "        # Adding experience into buffer\n",
    "        observations = env.get_state()[next_active].observation\n",
    "        exp = {'inputs': TrainNet.preprocess(prev_observations), 'a': action, 'r': reward, 'inputs2': TrainNet.preprocess(observations), 'done': env.game_over()}\n",
    "        TrainNet.add_experience(exp)\n",
    "\n",
    "        turns += 1\n",
    "        total_turns = Global_Step_Counter+turns\n",
    "        # Train the training model by using experiences in buffer and the target model\n",
    "        if total_turns%Steps_Till_Backprop==0:\n",
    "            TrainNet.train(TargetNet)\n",
    "        if total_turns%copy_step==0:\n",
    "            # Update the weights of the target model when reaching enough \"copy step\"\n",
    "            TargetNet.copy_weights(TrainNet)\n",
    "    results={key:[] for key in Metric_Titles}\n",
    "    for metric_name in Metric_Titles:\n",
    "        results[metric_name]=np.mean(Metric_Buffer[metric_name])\n",
    "    return results, turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T09:38:51.772671Z",
     "iopub.status.busy": "2023-11-04T09:38:51.772340Z",
     "iopub.status.idle": "2023-11-04T09:38:51.795153Z",
     "shell.execute_reply": "2023-11-04T09:38:51.794381Z",
     "shell.execute_reply.started": "2023-11-04T09:38:51.772623Z"
    }
   },
   "outputs": [],
   "source": [
    "env = ConnectX()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T09:38:52.132401Z",
     "iopub.status.busy": "2023-11-04T09:38:52.132051Z",
     "iopub.status.idle": "2023-11-04T09:38:52.138520Z",
     "shell.execute_reply": "2023-11-04T09:38:52.137604Z",
     "shell.execute_reply.started": "2023-11-04T09:38:52.132350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'timeout': 5, 'columns': 7, 'rows': 6, 'inarow': 4, 'steps': 1000}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T09:38:52.533083Z",
     "iopub.status.busy": "2023-11-04T09:38:52.532723Z",
     "iopub.status.idle": "2023-11-04T09:38:52.539122Z",
     "shell.execute_reply": "2023-11-04T09:38:52.538232Z",
     "shell.execute_reply.started": "2023-11-04T09:38:52.533035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.n, env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T09:38:53.092915Z",
     "iopub.status.busy": "2023-11-04T09:38:53.092529Z",
     "iopub.status.idle": "2023-11-04T09:38:53.108031Z",
     "shell.execute_reply": "2023-11-04T09:38:53.107065Z",
     "shell.execute_reply.started": "2023-11-04T09:38:53.092842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'action': 0,\n",
       "  'reward': 0.5,\n",
       "  'info': {},\n",
       "  'observation': {'board': [0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0],\n",
       "   'mark': 1},\n",
       "  'status': 'ACTIVE'},\n",
       " {'action': 0,\n",
       "  'reward': 0.5,\n",
       "  'info': {},\n",
       "  'observation': {'board': [0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0],\n",
       "   'mark': 2},\n",
       "  'status': 'INACTIVE'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T09:38:53.533379Z",
     "iopub.status.busy": "2023-11-04T09:38:53.533023Z",
     "iopub.status.idle": "2023-11-04T09:38:53.539859Z",
     "shell.execute_reply": "2023-11-04T09:38:53.538615Z",
     "shell.execute_reply.started": "2023-11-04T09:38:53.533336Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.current_player()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T09:38:53.612838Z",
     "iopub.status.busy": "2023-11-04T09:38:53.612484Z",
     "iopub.status.idle": "2023-11-04T09:38:53.619792Z",
     "shell.execute_reply": "2023-11-04T09:38:53.618967Z",
     "shell.execute_reply.started": "2023-11-04T09:38:53.612770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'board': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'mark': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_state()[0].observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T09:38:53.743908Z",
     "iopub.status.busy": "2023-11-04T09:38:53.743470Z",
     "iopub.status.idle": "2023-11-04T09:38:53.753742Z",
     "shell.execute_reply": "2023-11-04T09:38:53.752640Z",
     "shell.execute_reply.started": "2023-11-04T09:38:53.743833Z"
    }
   },
   "outputs": [],
   "source": [
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "TrainNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "TargetNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T09:38:54.252556Z",
     "iopub.status.busy": "2023-11-04T09:38:54.252223Z",
     "iopub.status.idle": "2023-11-04T09:38:54.264897Z",
     "shell.execute_reply": "2023-11-04T09:38:54.263960Z",
     "shell.execute_reply.started": "2023-11-04T09:38:54.252509Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations = env.get_state()[0].observation\n",
    "action, predictions = TrainNet.get_action(observations, epsilon)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T09:38:54.852608Z",
     "iopub.status.busy": "2023-11-04T09:38:54.852293Z",
     "iopub.status.idle": "2023-11-04T09:38:54.862831Z",
     "shell.execute_reply": "2023-11-04T09:38:54.861800Z",
     "shell.execute_reply.started": "2023-11-04T09:38:54.852562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'action': 1,\n",
       "  'reward': 0.5,\n",
       "  'info': {},\n",
       "  'observation': {'board': [0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    1,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0],\n",
       "   'mark': 1},\n",
       "  'status': 'INACTIVE'},\n",
       " {'action': 0,\n",
       "  'reward': 0.5,\n",
       "  'info': {},\n",
       "  'observation': {'board': [0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    1,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0],\n",
       "   'mark': 2},\n",
       "  'status': 'ACTIVE'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step([1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T09:38:55.503561Z",
     "iopub.status.busy": "2023-11-04T09:38:55.503181Z",
     "iopub.status.idle": "2023-11-04T09:38:55.513597Z",
     "shell.execute_reply": "2023-11-04T09:38:55.512741Z",
     "shell.execute_reply.started": "2023-11-04T09:38:55.503497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'action': 1,\n",
       "  'reward': 0.5,\n",
       "  'info': {},\n",
       "  'observation': {'board': [0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    2,\n",
       "    1,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0],\n",
       "   'mark': 1},\n",
       "  'status': 'ACTIVE'},\n",
       " {'action': 0,\n",
       "  'reward': 0.5,\n",
       "  'info': {},\n",
       "  'observation': {'board': [0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    2,\n",
       "    1,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0],\n",
       "   'mark': 2},\n",
       "  'status': 'INACTIVE'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step([1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training loop. Note here that we track some metrics (min/max/mean Q values). Although it may be hard to tell from these metrics what constitutes a good agent, tracking them can help you ascertain convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T09:38:56.572751Z",
     "iopub.status.busy": "2023-11-04T09:38:56.572420Z",
     "iopub.status.idle": "2023-11-04T09:38:56.577586Z",
     "shell.execute_reply": "2023-11-04T09:38:56.576705Z",
     "shell.execute_reply.started": "2023-11-04T09:38:56.572703Z"
    }
   },
   "outputs": [],
   "source": [
    "## hoho_test\n",
    "\n",
    "# num_states = env.observation_space.n\n",
    "# num_actions = env.action_space.n\n",
    "\n",
    "# states_next = env.get_state()[0].observation\n",
    "# states_next = torch.tensor(np.atleast_2d(TrainNet.preprocess(states_next)), dtype = torch.float32)\n",
    "# Move_Validity = states_next[:,:num_actions] == 0\n",
    "\n",
    "# print(f\"Move_Validity: {Move_Validity}\")\n",
    "\n",
    "# Next_Q_Values = TargetNet.predict(states_next)\n",
    "\n",
    "# print(f\"Next_Q_Values: {Next_Q_Values}\")\n",
    "\n",
    "# Next_Q_Values = np.where(Move_Validity,Next_Q_Values.detach().numpy(),Discard_Q_Value)\n",
    "\n",
    "# print(f\"Next_Q_Values2: {Next_Q_Values}\")\n",
    "\n",
    "# value_next = -np.max(Next_Q_Values,axis=1)\n",
    "\n",
    "# actual_values = np.where(dones, rewards, rewards+self.gamma*value_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T09:38:57.012662Z",
     "iopub.status.busy": "2023-11-04T09:38:57.012275Z",
     "iopub.status.idle": "2023-11-04T09:38:57.016685Z",
     "shell.execute_reply": "2023-11-04T09:38:57.015739Z",
     "shell.execute_reply.started": "2023-11-04T09:38:57.012589Z"
    }
   },
   "outputs": [],
   "source": [
    "# # hoho_test\n",
    "# states = env.get_state()[0].observation\n",
    "# states = torch.tensor(np.atleast_2d(TrainNet.preprocess(states)), dtype = torch.float32)\n",
    "# actions = torch.tensor(np.atleast_2d([2]), dtype = torch.long)\n",
    "# actions\n",
    "# selected_action_values = (TrainNet.predict(states) * torch.nn.functional.one_hot(actions, num_actions)).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T09:38:57.613208Z",
     "iopub.status.busy": "2023-11-04T09:38:57.612883Z",
     "iopub.status.idle": "2023-11-04T09:56:33.715127Z",
     "shell.execute_reply": "2023-11-04T09:56:33.713309Z",
     "shell.execute_reply.started": "2023-11-04T09:38:57.613160Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000 [00:00<?, ?it/s]\n",
      "  4%|▎         | 747/20000 [00:42<19:56, 16.08it/s, Steps=15600, Updates=1.25e+5]\n",
      "  4%|▍         | 804/20000 [00:46<18:50, 16.98it/s, Steps=16800, Updates=1.34e+5]19]\u001b[A\n",
      "  5%|▌         | 1056/20000 [01:00<15:50, 19.92it/s, Steps=22200, Updates=1.78e+5]] \u001b[A\n",
      "  7%|▋         | 1495/20000 [01:26<16:01, 19.25it/s, Steps=31400, Updates=2.51e+5]7]\u001b[A\n",
      "  8%|▊         | 1649/20000 [01:35<20:26, 14.97it/s, Steps=34800, Updates=2.78e+5]5]\u001b[A\n",
      "  9%|▉         | 1873/20000 [01:48<16:56, 17.83it/s, Steps=39600, Updates=3.17e+5]8]\u001b[A\n",
      " 10%|▉         | 1939/20000 [01:52<18:08, 16.59it/s, Steps=41000, Updates=3.28e+5]0]\u001b[A\n",
      " 10%|█         | 2005/20000 [01:56<19:24, 15.46it/s, Steps=42400, Updates=3.39e+5]9]\u001b[A\n",
      " 11%|█         | 2136/20000 [02:04<19:10, 15.53it/s, Steps=45200, Updates=3.62e+5]] \u001b[A\n",
      " 12%|█▏        | 2335/20000 [02:16<17:43, 16.60it/s, Steps=49800, Updates=3.98e+5] \u001b[A\n",
      " 12%|█▏        | 2380/20000 [02:19<19:00, 15.44it/s, Steps=50800, Updates=4.06e+5]7]\u001b[A\n",
      " 16%|█▌        | 3172/20000 [03:07<18:46, 14.93it/s, Steps=67800, Updates=5.42e+5]] \u001b[A\n",
      " 20%|██        | 4075/20000 [04:03<15:26, 17.18it/s, Steps=88200, Updates=7.06e+5]4]\u001b[A\n",
      " 21%|██        | 4163/20000 [04:08<19:04, 13.84it/s, Steps=90200, Updates=7.22e+5]8]\u001b[A\n",
      " 21%|██        | 4198/20000 [04:11<14:59, 17.57it/s, Steps=91000, Updates=7.28e+5]0]\u001b[A\n",
      " 21%|██▏       | 4293/20000 [04:16<17:19, 15.12it/s, Steps=93000, Updates=7.44e+5]] \u001b[A\n",
      " 23%|██▎       | 4516/20000 [04:30<13:47, 18.72it/s, Steps=98000, Updates=7.84e+5]1]\u001b[A\n",
      " 25%|██▍       | 4926/20000 [04:54<12:54, 19.46it/s, Steps=106200, Updates=8.5e+5]  \u001b[A\n",
      " 26%|██▌       | 5216/20000 [05:08<10:46, 22.88it/s, Steps=110800, Updates=8.86e+5]2]\u001b[A\n",
      " 27%|██▋       | 5405/20000 [05:18<11:24, 21.31it/s, Steps=113800, Updates=9.1e+5] 0]\u001b[A\n",
      " 28%|██▊       | 5513/20000 [05:23<13:52, 17.39it/s, Steps=115600, Updates=9.25e+5]] \u001b[A\n",
      " 30%|███       | 6010/20000 [05:48<11:03, 21.07it/s, Steps=123800, Updates=9.9e+5]  \u001b[A\n",
      " 31%|███       | 6142/20000 [05:55<09:50, 23.45it/s, Steps=126000, Updates=1.01e+6]]\u001b[A\n",
      " 32%|███▏      | 6494/20000 [06:13<10:03, 22.38it/s, Steps=132000, Updates=1.06e+6]]\u001b[A\n",
      " 33%|███▎      | 6543/20000 [06:16<10:03, 22.28it/s, Steps=132800, Updates=1.06e+6] \u001b[A\n",
      " 33%|███▎      | 6555/20000 [06:16<11:29, 19.50it/s, Steps=133000, Updates=1.06e+6]]\u001b[A\n",
      " 34%|███▍      | 6846/20000 [06:31<12:33, 17.46it/s, Steps=137800, Updates=1.1e+6]  \u001b[A\n",
      " 37%|███▋      | 7450/20000 [07:02<09:43, 21.50it/s, Steps=147800, Updates=1.18e+6]\u001b[A\n",
      " 39%|███▊      | 7719/20000 [07:15<09:58, 20.51it/s, Steps=152200, Updates=1.22e+6]\u001b[A\n",
      " 40%|███▉      | 7921/20000 [07:26<10:33, 19.07it/s, Steps=155600, Updates=1.24e+6]\u001b[A\n",
      " 41%|████▏     | 8259/20000 [07:44<12:33, 15.57it/s, Steps=161600, Updates=1.29e+6]]\u001b[A\n",
      " 43%|████▎     | 8657/20000 [08:05<14:00, 13.49it/s, Steps=168800, Updates=1.35e+6]]\u001b[A\n",
      " 44%|████▎     | 8731/20000 [08:10<09:47, 19.19it/s, Steps=170200, Updates=1.36e+6] \u001b[A\n",
      " 44%|████▍     | 8766/20000 [08:11<08:30, 22.02it/s, Steps=170800, Updates=1.37e+6]]\u001b[A\n",
      " 44%|████▍     | 8852/20000 [08:16<10:27, 17.77it/s, Steps=172200, Updates=1.38e+6]]\u001b[A\n",
      " 46%|████▌     | 9140/20000 [08:31<11:03, 16.38it/s, Steps=177200, Updates=1.42e+6]]\u001b[A\n",
      " 46%|████▋     | 9298/20000 [08:40<10:39, 16.74it/s, Steps=180400, Updates=1.44e+6]]\u001b[A\n",
      " 47%|████▋     | 9366/20000 [08:45<11:11, 15.84it/s, Steps=181800, Updates=1.45e+6]]\u001b[A\n",
      " 47%|████▋     | 9384/20000 [08:46<11:43, 15.09it/s, Steps=182200, Updates=1.46e+6] \u001b[A\n",
      " 49%|████▉     | 9846/20000 [09:13<10:38, 15.89it/s, Steps=191400, Updates=1.53e+6]\u001b[A\n",
      " 49%|████▉     | 9854/20000 [09:13<12:09, 13.92it/s, Steps=191600, Updates=1.53e+6]\u001b[A\n",
      " 50%|████▉     | 9907/20000 [09:17<13:58, 12.04it/s, Steps=192800, Updates=1.54e+6]]\u001b[A\n",
      " 51%|█████     | 10206/20000 [09:35<10:06, 16.14it/s, Steps=198800, Updates=1.59e+6]\u001b[A\n",
      " 53%|█████▎    | 10636/20000 [10:00<09:18, 16.77it/s, Steps=206800, Updates=1.65e+6]\u001b[A\n",
      " 53%|█████▎    | 10645/20000 [10:00<10:12, 15.26it/s, Steps=207000, Updates=1.66e+6]\u001b[A\n",
      " 54%|█████▍    | 10793/20000 [10:09<07:51, 19.53it/s, Steps=209800, Updates=1.68e+6]\u001b[A\n",
      " 55%|█████▍    | 10954/20000 [10:18<07:13, 20.86it/s, Steps=212800, Updates=1.7e+6] [A\n",
      " 56%|█████▌    | 11203/20000 [10:31<06:46, 21.66it/s, Steps=217200, Updates=1.74e+6][A\n",
      " 60%|█████▉    | 11905/20000 [11:10<08:22, 16.12it/s, Steps=229800, Updates=1.84e+6][A\n",
      " 60%|█████▉    | 11958/20000 [11:13<07:41, 17.43it/s, Steps=230800, Updates=1.85e+6][A\n",
      " 60%|█████▉    | 11997/20000 [11:14<07:20, 18.17it/s, Steps=231400, Updates=1.85e+6][A\n",
      " 60%|██████    | 12069/20000 [11:18<06:21, 20.80it/s, Steps=232600, Updates=1.86e+6]\u001b[A\n",
      " 62%|██████▏   | 12408/20000 [11:36<06:56, 18.24it/s, Steps=238400, Updates=1.91e+6]\u001b[A\n",
      " 62%|██████▏   | 12431/20000 [11:37<06:51, 18.42it/s, Steps=238800, Updates=1.91e+6][A\n",
      " 64%|██████▍   | 12821/20000 [11:57<05:35, 21.39it/s, Steps=245400, Updates=1.96e+6][A\n",
      " 65%|██████▍   | 12979/20000 [12:05<06:18, 18.53it/s, Steps=248000, Updates=1.98e+6][A\n",
      " 65%|██████▌   | 13017/20000 [12:07<05:32, 21.03it/s, Steps=248600, Updates=1.99e+6]A\n",
      " 68%|██████▊   | 13662/20000 [12:40<04:52, 21.68it/s, Steps=259200, Updates=2.07e+6][A\n",
      " 69%|██████▉   | 13822/20000 [12:48<04:56, 20.84it/s, Steps=261800, Updates=2.09e+6][A\n",
      " 69%|██████▉   | 13881/20000 [12:50<04:08, 24.66it/s, Steps=262600, Updates=2.1e+6] [A\n",
      " 70%|██████▉   | 13905/20000 [12:51<04:21, 23.30it/s, Steps=263000, Updates=2.1e+6]\u001b[A\n",
      " 70%|███████   | 14082/20000 [12:59<05:03, 19.51it/s, Steps=265400, Updates=2.12e+6][A\n",
      " 71%|███████   | 14132/20000 [13:01<04:09, 23.48it/s, Steps=266200, Updates=2.13e+6]A\n",
      " 71%|███████   | 14172/20000 [13:03<04:18, 22.58it/s, Steps=266800, Updates=2.13e+6]A\n",
      " 72%|███████▏  | 14301/20000 [13:09<05:37, 16.90it/s, Steps=268800, Updates=2.15e+6]A\n",
      " 73%|███████▎  | 14529/20000 [13:21<05:01, 18.12it/s, Steps=272600, Updates=2.18e+6][A\n",
      " 73%|███████▎  | 14543/20000 [13:22<04:09, 21.88it/s, Steps=272800, Updates=2.18e+6][A\n",
      " 75%|███████▍  | 14959/20000 [13:44<04:37, 18.16it/s, Steps=280200, Updates=2.24e+6][A\n",
      " 76%|███████▌  | 15122/20000 [13:52<04:05, 19.84it/s, Steps=282800, Updates=2.26e+6][A\n",
      " 76%|███████▋  | 15292/20000 [14:01<04:04, 19.25it/s, Steps=285600, Updates=2.28e+6]A\n",
      " 77%|███████▋  | 15418/20000 [14:06<03:07, 24.48it/s, Steps=287400, Updates=2.3e+6] A\n",
      " 79%|███████▉  | 15889/20000 [14:28<03:20, 20.50it/s, Steps=294600, Updates=2.36e+6]A\n",
      " 80%|███████▉  | 15915/20000 [14:30<03:04, 22.13it/s, Steps=295000, Updates=2.36e+6][A\n",
      " 80%|███████▉  | 15966/20000 [14:32<03:13, 20.89it/s, Steps=295800, Updates=2.37e+6][A\n",
      " 80%|███████▉  | 15999/20000 [14:34<03:49, 17.47it/s, Steps=296200, Updates=2.37e+6][A\n",
      " 80%|████████  | 16043/20000 [14:36<03:25, 19.27it/s, Steps=296800, Updates=2.37e+6]\n",
      " 81%|████████  | 16191/20000 [14:43<03:18, 19.16it/s, Steps=299200, Updates=2.39e+6]A\n",
      " 81%|████████▏ | 16253/20000 [14:45<02:34, 24.28it/s, Steps=3e+5, Updates=2.4e+6]   [A\n",
      " 82%|████████▏ | 16340/20000 [14:49<02:20, 25.98it/s, Steps=301200, Updates=2.41e+6][A\n",
      " 82%|████████▏ | 16381/20000 [14:51<02:30, 24.10it/s, Steps=301800, Updates=2.41e+6]A\n",
      " 82%|████████▏ | 16460/20000 [14:54<02:39, 22.13it/s, Steps=303000, Updates=2.42e+6][A\n",
      " 82%|████████▏ | 16498/20000 [14:56<02:40, 21.79it/s, Steps=303600, Updates=2.43e+6][A\n",
      " 83%|████████▎ | 16577/20000 [15:00<03:17, 17.35it/s, Steps=304800, Updates=2.44e+6]A\n",
      " 84%|████████▎ | 16711/20000 [15:07<02:41, 20.32it/s, Steps=306800, Updates=2.45e+6]\n",
      " 86%|████████▌ | 17244/20000 [15:30<01:56, 23.60it/s, Steps=314200, Updates=2.51e+6][A\n",
      " 87%|████████▋ | 17474/20000 [15:39<01:58, 21.24it/s, Steps=317000, Updates=2.54e+6][A\n",
      " 90%|█████████ | 18065/20000 [16:03<01:24, 22.84it/s, Steps=324400, Updates=2.6e+6] A\n",
      " 91%|█████████ | 18181/20000 [16:07<01:01, 29.81it/s, Steps=325800, Updates=2.61e+6]A\n",
      " 91%|█████████ | 18223/20000 [16:09<01:35, 18.60it/s, Steps=326400, Updates=2.61e+6]A\n",
      " 92%|█████████▏| 18315/20000 [16:13<00:58, 28.96it/s, Steps=327400, Updates=2.62e+6]A\n",
      " 93%|█████████▎| 18538/20000 [16:22<01:14, 19.64it/s, Steps=330400, Updates=2.64e+6]A\n",
      " 93%|█████████▎| 18557/20000 [16:23<01:28, 16.27it/s, Steps=330800, Updates=2.65e+6]A\n",
      " 94%|█████████▎| 18720/20000 [16:30<01:00, 21.11it/s, Steps=333000, Updates=2.66e+6]A\n",
      " 94%|█████████▍| 18771/20000 [16:33<00:52, 23.19it/s, Steps=333800, Updates=2.67e+6]A\n",
      " 95%|█████████▍| 18998/20000 [16:42<01:04, 15.62it/s, Steps=336600, Updates=2.69e+6][A\n",
      " 97%|█████████▋| 19430/20000 [17:04<00:29, 19.05it/s, Steps=344000, Updates=2.75e+6][A\n",
      " 98%|█████████▊| 19588/20000 [17:13<00:23, 17.38it/s, Steps=346800, Updates=2.77e+6][A\n",
      " 98%|█████████▊| 19629/20000 [17:15<00:18, 19.56it/s, Steps=347600, Updates=2.78e+6][A\n",
      " 99%|█████████▊| 19740/20000 [17:21<00:13, 19.90it/s, Steps=349600, Updates=2.8e+6] [A\n",
      "100%|█████████▉| 19999/20000 [17:35<00:00, 18.64it/s, Steps=349600, Updates=2.8e+6]\u001b[A\n",
      "100%|██████████| 20000/20000 [17:35<00:00, 18.95it/s, Steps=349600, Updates=2.8e+6]\n"
     ]
    }
   ],
   "source": [
    "env = ConnectX()\n",
    "\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "Metrics = {key:[] for key in Metric_Titles} #Here we will store metrics for plotting after training\n",
    "Metrics_Buffer = {key:[] for key in Metric_Titles} #Downsampling buffer\n",
    "\n",
    "# Initialize models\n",
    "TrainNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "TargetNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "\n",
    "if Train:\n",
    "    Global_Step_Counter=0\n",
    "    pbar = tqdm(range(episodes))\n",
    "    pbar2 = tqdm()\n",
    "    for n in pbar:\n",
    "        epsilon = max(min_epsilon, epsilon * decay)\n",
    "        results, steps = play_game(env, TrainNet, TargetNet, epsilon, copy_step, Global_Step_Counter)\n",
    "        Global_Step_Counter += steps\n",
    "        for metric_name in Metric_Titles:\n",
    "            Metrics_Buffer[metric_name].append(results[metric_name])\n",
    "\n",
    "        if Global_Step_Counter%N_Downsampling_Episodes==0:\n",
    "            for metric_name in Metric_Titles: #Downsample our metrics from the buffer\n",
    "                Metrics[metric_name].append(np.mean(Metrics_Buffer[metric_name]))\n",
    "                Metrics_Buffer[metric_name].clear()\n",
    "            pbar.set_postfix({\n",
    "                'Steps': Global_Step_Counter,\n",
    "                'Updates': Global_Step_Counter*batch_size/Steps_Till_Backprop\n",
    "            })\n",
    "\n",
    "            pbar2.set_postfix({\n",
    "                'max_Q': Metrics['Max_Q'][-1],\n",
    "                'avg_Q': Metrics['Avg_Q'][-1],\n",
    "                'min_Q': Metrics['Min_Q'][-1],\n",
    "                'epsilon': epsilon,\n",
    "                'turns': steps\n",
    "            })\n",
    "\n",
    "    def Plot(data,title):\n",
    "        plt.figure()\n",
    "        plt.plot(data)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel(title)\n",
    "        plt.savefig(title+'.png')\n",
    "        plt.close()\n",
    "\n",
    "    for metric_name in Metric_Titles:\n",
    "        Plot(Metrics[metric_name],metric_name)\n",
    "\n",
    "#     TrainNet.save_weights('./weights.h5')\n",
    "    TrainNet.save_weights('./weights.pth')\n",
    "else:\n",
    "#     TrainNet.load_weights('./weights.h5')\n",
    "    TrainNet.load_weights('./weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write our agent into a submission.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layers = []\n",
    "\n",
    "# Get all hidden layers' weights\n",
    "for i in range(len(hidden_units)):\n",
    "    fc_layers.extend([\n",
    "        TrainNet.model.hidden_layers[i].weights[0].numpy().tolist(), # weights\n",
    "        TrainNet.model.hidden_layers[i].weights[1].numpy().tolist() # bias\n",
    "    ])\n",
    "\n",
    "# Get output layer's weights\n",
    "fc_layers.extend([\n",
    "    TrainNet.model.output_layer.weights[0].numpy().tolist(), # weights\n",
    "    TrainNet.model.output_layer.weights[1].numpy().tolist() # bias\n",
    "])\n",
    "\n",
    "# Convert all layers into usable form before integrating to final agent\n",
    "fc_layers = list(map(\n",
    "    lambda x: str(list(np.round(x, precision))) \\\n",
    "        .replace('array(', '').replace(')', '') \\\n",
    "        .replace(' ', '') \\\n",
    "        .replace('\\n', ''),\n",
    "    fc_layers\n",
    "))\n",
    "fc_layers = np.reshape(fc_layers, (-1, 2))\n",
    "\n",
    "# Create the agent\n",
    "my_agent = '''def my_agent(observation, configuration):\n",
    "    import numpy as np\n",
    "\n",
    "'''\n",
    "\n",
    "# Write hidden layers\n",
    "for i, (w, b) in enumerate(fc_layers[:-1]):\n",
    "    my_agent += '    hl{}_w = np.array({}, dtype=np.float32)\\n'.format(i+1, w)\n",
    "    my_agent += '    hl{}_b = np.array({}, dtype=np.float32)\\n'.format(i+1, b)\n",
    "# Write output layer\n",
    "my_agent += '    ol_w = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][0])\n",
    "my_agent += '    ol_b = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][1])\n",
    "\n",
    "my_agent += '''\n",
    "    board = observation.board[:]\n",
    "    out = np.array([1 if val==observation.mark else 0 if val==0 else -1 for val in board],np.float32)\n",
    "'''\n",
    "\n",
    "# Calculate hidden layers\n",
    "for i in range(len(fc_layers[:-1])):\n",
    "    my_agent += '    out = np.matmul(out, hl{0}_w) + hl{0}_b\\n'.format(i+1)\n",
    "    my_agent += '    out = np.maximum(0,out)\\n' # Relu function\n",
    "# Calculate output layer\n",
    "my_agent += '    out = np.matmul(out, ol_w) + ol_b\\n'\n",
    "my_agent += '    out = np.tanh(out)\\n'\n",
    "\n",
    "my_agent += '''\n",
    "    for i in range(configuration.columns):\n",
    "        if observation.board[i] != 0:\n",
    "            out[i] = -1e7\n",
    "\n",
    "    return int(np.argmax(out))\n",
    "    '''\n",
    "\n",
    "with open('submission.py', 'w') as f:\n",
    "    f.write(my_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test our agent against the builtin Negamax opponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submission import my_agent\n",
    "\n",
    "def epsilon_greedify(agent,epsilon=0.05): #Greedify our agent so we don't play the same games over and over in strength evaluation\n",
    "    def greedified_agent(observation,configuration):\n",
    "        import random\n",
    "        if random.random()<epsilon:\n",
    "            return random.choice([i for i in range(num_actions) if observation.board[i]==0])\n",
    "        else:\n",
    "            return agent(observation,configuration)\n",
    "    return greedified_agent\n",
    "\n",
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\n",
    "\n",
    "my_agent = epsilon_greedify(my_agent)\n",
    "print(\"My Agent vs. Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=20)))\n",
    "print(\"Negamax Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", my_agent], num_episodes=20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "t = np.array([1, 2, 3, 4])\n",
    "np.max(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.max(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = torch.tensor([[1, 2, 3, 4], [2, 3, 4, 10], [1, 42, 2, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4],\n",
       "        [10],\n",
       "        [42]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs.max(dim = 1)[0].view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10000000., -10000000., -10000000., -10000000.],\n",
       "        [-10000000., -10000000., -10000000., -10000000.],\n",
       "        [-10000000., -10000000., -10000000., -10000000.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = -1e7\n",
    "torch.full((3, 4), val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
